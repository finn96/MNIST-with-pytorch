{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels)\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')\n",
    "print(train_labels[z][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782,100) # 80\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = F.max_pool1d(F.sigmoid(self.conv1(x)), (2))  ## TODO: Test this again against whatever you leave here\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        return self.fc2(x)\n",
    "        #return self.fc3(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# TODO:!DF! test with and without these weights\n",
    "net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc2.bias.data\n",
    "#net.fc3.weight.data.uniform_(-1.0/np.sqrt(80),1.0/np.sqrt(80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, running_loss: 0.31329599022865295\n",
      "Epoch: 0, Batch: 2000, running_loss: 72.33659165352583\n",
      "Epoch: 0, Batch: 4000, running_loss: 56.24923073500395\n",
      "Epoch: 0, Batch: 6000, running_loss: 50.42842769622803\n",
      "Epoch: 0, Batch: 8000, running_loss: 48.0273976624012\n",
      "Epoch: 0, Batch: 10000, running_loss: 46.74437516927719\n",
      "Epoch: 0, Batch: 12000, running_loss: 45.475361190736294\n",
      "Epoch: 0, Batch: 14000, running_loss: 44.94332781434059\n",
      "Epoch: 0, Batch: 16000, running_loss: 44.45939531177282\n",
      "Epoch: 0, Batch: 18000, running_loss: 44.00933165103197\n",
      "Epoch: 0, Batch: 20000, running_loss: 43.72081562876701\n",
      "Epoch: 0, Batch: 22000, running_loss: 43.29031403362751\n",
      "Epoch: 0, Batch: 24000, running_loss: 43.078285947442055\n",
      "Epoch: 0, Batch: 26000, running_loss: 42.468748204410076\n",
      "Epoch: 0, Batch: 28000, running_loss: 42.31836646050215\n",
      "Epoch: 0, Batch: 30000, running_loss: 42.01204485446215\n",
      "Epoch: 0, Batch: 32000, running_loss: 42.137769505381584\n",
      "Epoch: 0, Batch: 34000, running_loss: 41.63704100996256\n",
      "Epoch: 0, Batch: 36000, running_loss: 41.43003844469786\n",
      "Epoch: 0, Batch: 38000, running_loss: 41.180981662124395\n",
      "Epoch: 0, Batch: 40000, running_loss: 40.327593833208084\n",
      "Epoch: 0, Batch: 42000, running_loss: 40.43998933956027\n",
      "Epoch: 0, Batch: 44000, running_loss: 40.579087391495705\n",
      "Epoch: 0, Batch: 46000, running_loss: 39.98365078866482\n",
      "Epoch: 0, Batch: 48000, running_loss: 39.628109864890575\n",
      "Epoch: 0, Batch: 50000, running_loss: 39.61249243095517\n",
      "Epoch: 0, Batch: 52000, running_loss: 39.244454603642225\n",
      "Epoch: 0, Batch: 54000, running_loss: 39.118722308427095\n",
      "Epoch: 0, Batch: 56000, running_loss: 38.82339857891202\n",
      "Epoch: 0, Batch: 58000, running_loss: 37.921684451401234\n",
      "Epoch: 1, Batch: 0, running_loss: 0.0608525387942791\n",
      "Epoch: 1, Batch: 2000, running_loss: 37.86655047535896\n",
      "Epoch: 1, Batch: 4000, running_loss: 37.33501364290714\n",
      "Epoch: 1, Batch: 6000, running_loss: 37.197657991200686\n",
      "Epoch: 1, Batch: 8000, running_loss: 37.286420073360205\n",
      "Epoch: 1, Batch: 10000, running_loss: 36.550622172653675\n",
      "Epoch: 1, Batch: 12000, running_loss: 36.46650767326355\n",
      "Epoch: 1, Batch: 14000, running_loss: 36.55800177901983\n",
      "Epoch: 1, Batch: 16000, running_loss: 36.72181963175535\n",
      "Epoch: 1, Batch: 18000, running_loss: 36.639057233929634\n",
      "Epoch: 1, Batch: 20000, running_loss: 35.722375471144915\n",
      "Epoch: 1, Batch: 22000, running_loss: 35.39459014311433\n",
      "Epoch: 1, Batch: 24000, running_loss: 35.639767698943615\n",
      "Epoch: 1, Batch: 26000, running_loss: 35.028638638556004\n",
      "Epoch: 1, Batch: 28000, running_loss: 35.00797997042537\n",
      "Epoch: 1, Batch: 30000, running_loss: 34.83911648578942\n",
      "Epoch: 1, Batch: 32000, running_loss: 35.34692661464214\n",
      "Epoch: 1, Batch: 34000, running_loss: 35.06723338738084\n",
      "Epoch: 1, Batch: 36000, running_loss: 34.58731432631612\n",
      "Epoch: 1, Batch: 38000, running_loss: 34.3144424110651\n",
      "Epoch: 1, Batch: 40000, running_loss: 34.158047791570425\n",
      "Epoch: 1, Batch: 42000, running_loss: 33.71084958687425\n",
      "Epoch: 1, Batch: 44000, running_loss: 33.949331913143396\n",
      "Epoch: 1, Batch: 46000, running_loss: 33.69039787724614\n",
      "Epoch: 1, Batch: 48000, running_loss: 33.16727628931403\n",
      "Epoch: 1, Batch: 50000, running_loss: 33.53166022896767\n",
      "Epoch: 1, Batch: 52000, running_loss: 32.426251001656055\n",
      "Epoch: 1, Batch: 54000, running_loss: 32.8116392493248\n",
      "Epoch: 1, Batch: 56000, running_loss: 32.48786321654916\n",
      "Epoch: 1, Batch: 58000, running_loss: 31.967113312333822\n",
      "Epoch: 2, Batch: 0, running_loss: 0.05551966279745102\n",
      "Epoch: 2, Batch: 2000, running_loss: 32.183777660131454\n",
      "Epoch: 2, Batch: 4000, running_loss: 31.3942171279341\n",
      "Epoch: 2, Batch: 6000, running_loss: 31.644594356417656\n",
      "Epoch: 2, Batch: 8000, running_loss: 31.675154564902186\n",
      "Epoch: 2, Batch: 10000, running_loss: 31.19740508683026\n",
      "Epoch: 2, Batch: 12000, running_loss: 31.422320991754532\n",
      "Epoch: 2, Batch: 14000, running_loss: 31.661291241645813\n",
      "Epoch: 2, Batch: 16000, running_loss: 31.564051190391183\n",
      "Epoch: 2, Batch: 18000, running_loss: 31.599268628284335\n",
      "Epoch: 2, Batch: 20000, running_loss: 30.453266421332955\n",
      "Epoch: 2, Batch: 22000, running_loss: 30.225512843579054\n",
      "Epoch: 2, Batch: 24000, running_loss: 30.681633522734046\n",
      "Epoch: 2, Batch: 26000, running_loss: 30.16895237378776\n",
      "Epoch: 2, Batch: 28000, running_loss: 30.08741906285286\n",
      "Epoch: 2, Batch: 30000, running_loss: 30.114222398027778\n",
      "Epoch: 2, Batch: 32000, running_loss: 30.35076235421002\n",
      "Epoch: 2, Batch: 34000, running_loss: 30.121827425435185\n",
      "Epoch: 2, Batch: 36000, running_loss: 30.106528002768755\n",
      "Epoch: 2, Batch: 38000, running_loss: 30.114876970648766\n",
      "Epoch: 2, Batch: 40000, running_loss: 29.49401736073196\n",
      "Epoch: 2, Batch: 42000, running_loss: 28.894006218761206\n",
      "Epoch: 2, Batch: 44000, running_loss: 29.67089184373617\n",
      "Epoch: 2, Batch: 46000, running_loss: 29.04498765990138\n",
      "Epoch: 2, Batch: 48000, running_loss: 28.6254302226007\n",
      "Epoch: 2, Batch: 50000, running_loss: 28.957911966368556\n",
      "Epoch: 2, Batch: 52000, running_loss: 28.23364482820034\n",
      "Epoch: 2, Batch: 54000, running_loss: 28.48908283933997\n",
      "Epoch: 2, Batch: 56000, running_loss: 28.084338158369064\n",
      "Epoch: 2, Batch: 58000, running_loss: 27.848573764786124\n",
      "Epoch: 3, Batch: 0, running_loss: 0.03979330137372017\n",
      "Epoch: 3, Batch: 2000, running_loss: 28.747555484063923\n",
      "Epoch: 3, Batch: 4000, running_loss: 27.600786224007607\n",
      "Epoch: 3, Batch: 6000, running_loss: 27.35744842980057\n",
      "Epoch: 3, Batch: 8000, running_loss: 27.75059737637639\n",
      "Epoch: 3, Batch: 10000, running_loss: 27.39328406844288\n",
      "Epoch: 3, Batch: 12000, running_loss: 27.654389837756753\n",
      "Epoch: 3, Batch: 14000, running_loss: 28.073115095496178\n",
      "Epoch: 3, Batch: 16000, running_loss: 27.86469830200076\n",
      "Epoch: 3, Batch: 18000, running_loss: 28.012133795768023\n",
      "Epoch: 3, Batch: 20000, running_loss: 26.56293378584087\n",
      "Epoch: 3, Batch: 22000, running_loss: 26.391170509159565\n",
      "Epoch: 3, Batch: 24000, running_loss: 27.522532895207405\n",
      "Epoch: 3, Batch: 26000, running_loss: 26.57300023548305\n",
      "Epoch: 3, Batch: 28000, running_loss: 26.786618831567466\n",
      "Epoch: 3, Batch: 30000, running_loss: 26.625579157844186\n",
      "Epoch: 3, Batch: 32000, running_loss: 27.33004017546773\n",
      "Epoch: 3, Batch: 34000, running_loss: 26.586067443713546\n",
      "Epoch: 3, Batch: 36000, running_loss: 26.67796771414578\n",
      "Epoch: 3, Batch: 38000, running_loss: 26.887659720145166\n",
      "Epoch: 3, Batch: 40000, running_loss: 26.2328735049814\n",
      "Epoch: 3, Batch: 42000, running_loss: 26.030128200538456\n",
      "Epoch: 3, Batch: 44000, running_loss: 26.188451776281\n",
      "Epoch: 3, Batch: 46000, running_loss: 25.97739146091044\n",
      "Epoch: 3, Batch: 48000, running_loss: 26.052668230608106\n",
      "Epoch: 3, Batch: 50000, running_loss: 25.47996844816953\n",
      "Epoch: 3, Batch: 52000, running_loss: 24.805650459602475\n",
      "Epoch: 3, Batch: 54000, running_loss: 25.676052902825177\n",
      "Epoch: 3, Batch: 56000, running_loss: 25.36787101253867\n",
      "Epoch: 3, Batch: 58000, running_loss: 24.65737493429333\n",
      "Epoch: 4, Batch: 0, running_loss: 0.047462206333875656\n",
      "Epoch: 4, Batch: 2000, running_loss: 25.511720214039087\n",
      "Epoch: 4, Batch: 4000, running_loss: 24.348958819638938\n",
      "Epoch: 4, Batch: 6000, running_loss: 25.246551451273263\n",
      "Epoch: 4, Batch: 8000, running_loss: 25.16451510414481\n",
      "Epoch: 4, Batch: 10000, running_loss: 24.876631774939597\n",
      "Epoch: 4, Batch: 12000, running_loss: 24.96473646070808\n",
      "Epoch: 4, Batch: 14000, running_loss: 24.934327215887606\n",
      "Epoch: 4, Batch: 16000, running_loss: 25.346668602898717\n",
      "Epoch: 4, Batch: 18000, running_loss: 25.28444218542427\n",
      "Epoch: 4, Batch: 20000, running_loss: 24.063824250362813\n",
      "Epoch: 4, Batch: 22000, running_loss: 23.693798480555415\n",
      "Epoch: 4, Batch: 24000, running_loss: 24.974917085841298\n",
      "Epoch: 4, Batch: 26000, running_loss: 24.06444218987599\n",
      "Epoch: 4, Batch: 28000, running_loss: 24.061774177476764\n",
      "Epoch: 4, Batch: 30000, running_loss: 24.50306094903499\n",
      "Epoch: 4, Batch: 32000, running_loss: 24.998985721729696\n",
      "Epoch: 4, Batch: 34000, running_loss: 24.301178227178752\n",
      "Epoch: 4, Batch: 36000, running_loss: 23.920475363731384\n",
      "Epoch: 4, Batch: 38000, running_loss: 24.904882760718465\n",
      "Epoch: 4, Batch: 40000, running_loss: 24.3736866498366\n",
      "Epoch: 4, Batch: 42000, running_loss: 24.370912393555045\n",
      "Epoch: 4, Batch: 44000, running_loss: 23.96149160992354\n",
      "Epoch: 4, Batch: 46000, running_loss: 23.797618340700865\n",
      "Epoch: 4, Batch: 48000, running_loss: 23.431513563729823\n",
      "Epoch: 4, Batch: 50000, running_loss: 23.580837989225984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Batch: 52000, running_loss: 22.738422139547765\n",
      "Epoch: 4, Batch: 54000, running_loss: 23.411032343283296\n",
      "Epoch: 4, Batch: 56000, running_loss: 22.76124620717019\n",
      "Epoch: 4, Batch: 58000, running_loss: 22.81079941149801\n",
      "Epoch: 5, Batch: 0, running_loss: 0.04604004696011543\n",
      "Epoch: 5, Batch: 2000, running_loss: 23.43276694137603\n",
      "Epoch: 5, Batch: 4000, running_loss: 22.20535835949704\n",
      "Epoch: 5, Batch: 6000, running_loss: 23.207819934003055\n",
      "Epoch: 5, Batch: 8000, running_loss: 23.147045145742595\n",
      "Epoch: 5, Batch: 10000, running_loss: 23.27639612648636\n",
      "Epoch: 5, Batch: 12000, running_loss: 22.828903317451477\n",
      "Epoch: 5, Batch: 14000, running_loss: 23.38077324256301\n",
      "Epoch: 5, Batch: 16000, running_loss: 23.84646950662136\n",
      "Epoch: 5, Batch: 18000, running_loss: 23.890631233341992\n",
      "Epoch: 5, Batch: 20000, running_loss: 21.79438308533281\n",
      "Epoch: 5, Batch: 22000, running_loss: 22.099273666739464\n",
      "Epoch: 5, Batch: 24000, running_loss: 23.26460809353739\n",
      "Epoch: 5, Batch: 26000, running_loss: 22.405182969756424\n",
      "Epoch: 5, Batch: 28000, running_loss: 22.49735670303926\n",
      "Epoch: 5, Batch: 30000, running_loss: 22.738246262539178\n",
      "Epoch: 5, Batch: 32000, running_loss: 23.546773907728493\n",
      "Epoch: 5, Batch: 34000, running_loss: 22.58717840537429\n",
      "Epoch: 5, Batch: 36000, running_loss: 22.317763289436698\n",
      "Epoch: 5, Batch: 38000, running_loss: 22.307257710024714\n",
      "Epoch: 5, Batch: 40000, running_loss: 22.476436221040785\n",
      "Epoch: 5, Batch: 42000, running_loss: 21.981750570703298\n",
      "Epoch: 5, Batch: 44000, running_loss: 22.592871739529073\n",
      "Epoch: 5, Batch: 46000, running_loss: 22.59736498631537\n",
      "Epoch: 5, Batch: 48000, running_loss: 22.13028704095632\n",
      "Epoch: 5, Batch: 50000, running_loss: 21.51186584541574\n",
      "Epoch: 5, Batch: 52000, running_loss: 21.442152187693864\n",
      "Epoch: 5, Batch: 54000, running_loss: 21.836247727274895\n",
      "Epoch: 5, Batch: 56000, running_loss: 21.13361813640222\n",
      "Epoch: 5, Batch: 58000, running_loss: 20.956008557230234\n",
      "Epoch: 6, Batch: 0, running_loss: 0.023821767419576645\n",
      "Epoch: 6, Batch: 2000, running_loss: 21.95073035499081\n",
      "Epoch: 6, Batch: 4000, running_loss: 20.734592204680666\n",
      "Epoch: 6, Batch: 6000, running_loss: 21.682216755114496\n",
      "Epoch: 6, Batch: 8000, running_loss: 21.633311765268445\n",
      "Epoch: 6, Batch: 10000, running_loss: 21.37875420693308\n",
      "Epoch: 6, Batch: 12000, running_loss: 21.640017380937934\n",
      "Epoch: 6, Batch: 14000, running_loss: 22.1113004013896\n",
      "Epoch: 6, Batch: 16000, running_loss: 22.388032938353717\n",
      "Epoch: 6, Batch: 18000, running_loss: 21.759736279025674\n",
      "Epoch: 6, Batch: 20000, running_loss: 20.484742345288396\n",
      "Epoch: 6, Batch: 22000, running_loss: 20.69848100282252\n",
      "Epoch: 6, Batch: 24000, running_loss: 21.73330250568688\n",
      "Epoch: 6, Batch: 26000, running_loss: 21.09215462487191\n",
      "Epoch: 6, Batch: 28000, running_loss: 20.54001906933263\n",
      "Epoch: 6, Batch: 30000, running_loss: 21.505092880222946\n",
      "Epoch: 6, Batch: 32000, running_loss: 22.326955227181315\n",
      "Epoch: 6, Batch: 34000, running_loss: 21.268990920856595\n",
      "Epoch: 6, Batch: 36000, running_loss: 20.820834073238075\n",
      "Epoch: 6, Batch: 38000, running_loss: 21.50698033720255\n",
      "Epoch: 6, Batch: 40000, running_loss: 20.9179689893499\n",
      "Epoch: 6, Batch: 42000, running_loss: 20.664595114067197\n",
      "Epoch: 6, Batch: 44000, running_loss: 21.206453661900014\n",
      "Epoch: 6, Batch: 46000, running_loss: 21.45336255338043\n",
      "Epoch: 6, Batch: 48000, running_loss: 20.685775560792536\n",
      "Epoch: 6, Batch: 50000, running_loss: 21.074514356441796\n",
      "Epoch: 6, Batch: 52000, running_loss: 20.285344878211617\n",
      "Epoch: 6, Batch: 54000, running_loss: 20.394882382359356\n",
      "Epoch: 6, Batch: 56000, running_loss: 19.93819678062573\n",
      "Epoch: 6, Batch: 58000, running_loss: 20.053975569084287\n",
      "Epoch: 7, Batch: 0, running_loss: 0.024291861802339554\n",
      "Epoch: 7, Batch: 2000, running_loss: 20.717113133519888\n",
      "Epoch: 7, Batch: 4000, running_loss: 19.78250158485025\n",
      "Epoch: 7, Batch: 6000, running_loss: 20.240810328628868\n",
      "Epoch: 7, Batch: 8000, running_loss: 20.568417889531702\n",
      "Epoch: 7, Batch: 10000, running_loss: 20.525854281615466\n",
      "Epoch: 7, Batch: 12000, running_loss: 20.125549332238734\n",
      "Epoch: 7, Batch: 14000, running_loss: 20.926287519279867\n",
      "Epoch: 7, Batch: 16000, running_loss: 21.039070532657206\n",
      "Epoch: 7, Batch: 18000, running_loss: 20.812645650468767\n",
      "Epoch: 7, Batch: 20000, running_loss: 19.678970226086676\n",
      "Epoch: 7, Batch: 22000, running_loss: 20.22187434276566\n",
      "Epoch: 7, Batch: 24000, running_loss: 20.737500789109617\n",
      "Epoch: 7, Batch: 26000, running_loss: 20.50382666569203\n",
      "Epoch: 7, Batch: 28000, running_loss: 20.247527221683413\n",
      "Epoch: 7, Batch: 30000, running_loss: 20.41442718380131\n",
      "Epoch: 7, Batch: 32000, running_loss: 21.811600991524756\n",
      "Epoch: 7, Batch: 34000, running_loss: 20.68455031234771\n",
      "Epoch: 7, Batch: 36000, running_loss: 20.232528957538307\n",
      "Epoch: 7, Batch: 38000, running_loss: 20.600405879318714\n",
      "Epoch: 7, Batch: 40000, running_loss: 19.907584007363766\n",
      "Epoch: 7, Batch: 42000, running_loss: 19.854042997583747\n",
      "Epoch: 7, Batch: 44000, running_loss: 20.412019290030003\n",
      "Epoch: 7, Batch: 46000, running_loss: 19.924859495833516\n",
      "Epoch: 7, Batch: 48000, running_loss: 19.732863477431238\n",
      "Epoch: 7, Batch: 50000, running_loss: 19.906757545657456\n",
      "Epoch: 7, Batch: 52000, running_loss: 19.250719531439245\n",
      "Epoch: 7, Batch: 54000, running_loss: 19.82558853388764\n",
      "Epoch: 7, Batch: 56000, running_loss: 19.40882044751197\n",
      "Epoch: 7, Batch: 58000, running_loss: 18.914484115317464\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "for epoch in range(8):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0,len(train_images), 4):\n",
    "        net.zero_grad()\n",
    "        out = net(train_images[i:i+4])\n",
    "        loss = criterion(out, train_labels[i:i+4])\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:\n",
    "            ## I recommend doing things like this. It helped me catch mistakes\n",
    "            #print(batch[0][0])\n",
    "            #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "            #plt.show()\n",
    "            #print(targets[0])\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * 0.01)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t_images[750], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7196 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = t_test_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-voice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
