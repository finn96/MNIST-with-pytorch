{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels).long()\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#_ = train_images[:][:].div_(255.0)  # Normalize the pixel values to between [0,1.0]\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,  18.,  18.,\n",
      "        126., 136., 175.,  26., 166., 255., 247., 127.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94., 154.,\n",
      "        170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,  64.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
      "        238., 253., 253., 253., 253., 253., 253., 253., 253., 251.,  93.,  82.,\n",
      "         82.,  56.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,  18., 219., 253., 253., 253., 253., 253., 198., 182.,\n",
      "        247., 241.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
      "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0., 139., 253., 190.,   2.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,\n",
      "        190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240., 253.,\n",
      "        253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0., 249., 253., 249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253., 250., 182.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,  24., 114., 221., 253., 253., 253.,\n",
      "        253., 201.,  78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213., 253.,\n",
      "        253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171.,\n",
      "        219., 253., 253., 253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         55., 172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,  16.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd670a5d310>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "print(train_images[0][0])\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)  # stride = 1\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782,100) # 80\n",
    "        #self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        #x = F.sigmoid(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = x.view(-1, x.shape[1:].numel())  # Linear input should be in shape [batch_size, features x height x width]\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        \n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        #x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        # What we're doing here is re-shaping x to remove the channel dimension. This was confusing for me to understand. But\n",
    "        # basically pytorch expects everything in mini batches. So incoming data is in the form (images_per_batch,\n",
    "        # channels_per_image, (h,w)). In our case, we have (4,1,784), because we have 4 images per batch, 1 channel per image,\n",
    "        # and a flat vector of length 784 representing that channel (and because we have only one channel, that image). Our\n",
    "        # output, however, is channel indepedent. We want our output to be in the shape (images_per_batch,\n",
    "        # {single_image_output_shape}). In that case, our final output shape is (4, 10)\n",
    "        return self.fc2(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%\n",
    "#https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + pytorch bias, Sigmoid, Kernel Size 3: 85%, 86%, 86%\n",
    "# 5 Epochs with pytorch weights/bias, Sigmoid, Kernel Size 3: 87%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + 0 bias, Sigmoid, Kernel Size 3: 86%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights and bias, Sigmoid, Kernel Size 3: 86%, 85%, 86%\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 3: 87%, 87%, 86%,(5) -- 90.35% 90.44%  (10) **\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 5: 86%, 86%, 86% (5) -- 89.31% 90.03%  (10)\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 8: 87%, 86%, 87% (5) -- 89.79% 88.58%  (10)\n",
    "\n",
    "## TODO: !DF! Best of the normalized Xavier head-to-head with pytorch defaults, 10 epochs, track loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "# TODO:!DF! test with and without these weights\n",
    "#net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc1.bias.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ =  net.fc2.bias.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))-np.sqrt(6.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch: 0, Batch: 5999, running_loss: 428598.3710746765'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-296-6b87c2a274ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;31m####f.data.sub_(f.grad.data * 0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#f.data.sub_(f.grad.data * 0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch-mnist/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \"\"\"\n\u001b[1;32m    941\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;31m# TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "criterion = nn.MSELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "mb_size = 4\n",
    "num_batches = len(train_images) // mb_size\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    #for i in range(0,len(train_images), 4):\n",
    "    for i in range(0, num_batches):\n",
    "        net.zero_grad()\n",
    "        out = net.forward(train_images[i*mb_size:(i+1)*mb_size])\n",
    "            #loss = criterion(out, train_labels[i:i+4])\n",
    "        loss = criterion2(out, t_labels[i*mb_size:(i+1)*mb_size])\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "                ## I recommend doing things like this. It helped me catch mistakes\n",
    "                #print(batch[0][0])\n",
    "                #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(targets[0])\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data = f.data - f.grad.data * .01\n",
    "                ####f.data.sub_(f.grad.data * 0.01)\n",
    "            #f.data.sub_(f.grad.data * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "motivated-difficulty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9454.6\n",
      "9480.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "default_wb_results = copy.deepcopy(training_outs)\n",
    "#norm_xav_wb_results = copy.deepcopy(training_outs)\n",
    "print(np.mean(norm_xav_wb_results))\n",
    "print(np.mean(default_wb_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "capital-forty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1536,  0.2882, -0.4127, -0.0651,  0.3566,  0.1120, -0.2545,  0.0706,\n",
       "         -0.7031, -0.1304],\n",
       "        [ 0.1242,  0.2807, -0.4189, -0.0767,  0.3486,  0.1419, -0.2370,  0.0854,\n",
       "         -0.6636, -0.1226],\n",
       "        [ 0.1348,  0.2714, -0.3890, -0.0232,  0.3746,  0.1413, -0.2490,  0.0728,\n",
       "         -0.6497, -0.1143],\n",
       "        [ 0.0769,  0.2961, -0.3989, -0.0508,  0.3697,  0.1184, -0.2663,  0.0687,\n",
       "         -0.6492, -0.1373]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-91ac2a9860c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9496\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "img = t_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1050,  0.4207,  0.8344,  8.9818, -2.0588,  2.4454, -7.6334, -3.6497,\n",
       "         2.2114, -0.3338], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9],\n",
       "        [3],\n",
       "        [8],\n",
       "        [2]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stone-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0344, -0.0227, -0.0099,  ...,  0.0357,  0.0121, -0.0129],\n",
       "        [-0.0289, -0.0260, -0.0018,  ...,  0.0205, -0.0217,  0.0199],\n",
       "        [ 0.0091,  0.0305,  0.0191,  ..., -0.0006,  0.0047, -0.0305],\n",
       "        ...,\n",
       "        [-0.0057,  0.0190, -0.0134,  ..., -0.0067,  0.0096, -0.0161],\n",
       "        [ 0.0202, -0.0128, -0.0338,  ..., -0.0176, -0.0176,  0.0211],\n",
       "        [-0.0263, -0.0035,  0.0092,  ..., -0.0211, -0.0109, -0.0111]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(782,100)\n",
    "ll2 = nn.Linear(782, 100)\n",
    "ll2.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "requested-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6307e-05)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "neither-marking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.7258e-05)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll2.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "southern-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "useful-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "threaded-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "christian-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[0][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "temporal-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "legitimate-morris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "scheduled-exchange",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "olive-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_labels.size()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "intensive-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "written-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[ 1.5822,  2.0050, -0.5929]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-1.1087], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0119, -0.0085,  0.0110,  ...,  0.0116, -0.0096, -0.0135],\n",
       "         [-0.0320,  0.0286, -0.0208,  ..., -0.0038, -0.0199, -0.0130],\n",
       "         [-0.0223,  0.0322,  0.0007,  ..., -0.0159, -0.0188, -0.0315],\n",
       "         ...,\n",
       "         [-0.0292,  0.0170,  0.0317,  ...,  0.0107, -0.0131, -0.0099],\n",
       "         [ 0.0201, -0.0065,  0.0112,  ..., -0.0329, -0.0110, -0.0011],\n",
       "         [-0.0252, -0.0218, -0.0027,  ...,  0.0336,  0.0312, -0.0325]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0291,  0.0315,  0.0305, -0.0010,  0.0330,  0.0426, -0.0345,  0.0205,\n",
       "         -0.0040,  0.0300, -0.0284,  0.0403, -0.0158,  0.0218,  0.0196, -0.0311,\n",
       "          0.0297,  0.0403, -0.0171,  0.0373,  0.0035,  0.0131,  0.0121, -0.0083,\n",
       "          0.0167, -0.0325, -0.0125,  0.0208, -0.0074,  0.0100,  0.0063,  0.0023,\n",
       "         -0.0191, -0.0409,  0.0187,  0.0119,  0.0230,  0.0542, -0.0017, -0.0182,\n",
       "         -0.0049, -0.0191, -0.0290,  0.0102, -0.0125, -0.0276,  0.0201,  0.0289,\n",
       "          0.0089,  0.0384,  0.0261,  0.0032,  0.0115,  0.0233, -0.0097, -0.0288,\n",
       "         -0.0247, -0.0021, -0.0066,  0.0253, -0.0046, -0.0194,  0.0415,  0.0067,\n",
       "         -0.0154, -0.0367,  0.0342,  0.0213, -0.0159, -0.0201,  0.0397, -0.0207,\n",
       "         -0.0295, -0.0045,  0.0093,  0.0454, -0.0382, -0.0282,  0.0014,  0.0213,\n",
       "          0.0137, -0.0092,  0.0209,  0.0034, -0.0119, -0.0071, -0.0023, -0.0114,\n",
       "         -0.0173, -0.0470,  0.0109, -0.0405,  0.0175,  0.0137,  0.0277,  0.0030,\n",
       "          0.0244, -0.0079,  0.0283, -0.0333], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 3.0258e-03,  5.2314e-02, -4.5423e-02,  1.5536e-01,  4.6427e-02,\n",
       "          -2.5485e-01,  2.1412e-02, -7.9930e-03,  1.4188e-01,  4.3846e-01,\n",
       "          -1.9172e-01, -1.5980e-01,  5.2889e-02,  5.1682e-02, -2.0219e-02,\n",
       "           7.4584e-02,  1.7887e-01, -3.9276e-02,  2.7752e-01, -5.3229e-02,\n",
       "          -9.7377e-03,  1.3718e-02, -1.5890e-01,  2.2185e-01,  1.2075e-01,\n",
       "           5.0346e-01, -6.2454e-02, -2.2633e-01,  1.6712e-02,  3.1893e-01,\n",
       "           5.3911e-02, -3.5876e-01, -1.3212e-01,  2.6533e-01, -4.5288e-01,\n",
       "          -1.9184e-01,  1.5420e-01, -6.8150e-02, -9.7781e-02, -3.3338e-01,\n",
       "          -3.1908e-01,  2.4769e-01, -1.0298e-01, -1.6138e-01, -3.2818e-01,\n",
       "          -3.1589e-01, -2.5064e-03, -4.4193e-01, -3.3526e-01, -5.9692e-02,\n",
       "           1.6478e-01, -2.5615e-01, -3.2051e-02,  4.6534e-01,  2.3161e-01,\n",
       "           1.3709e-01,  3.8667e-01,  1.7615e-01,  2.3169e-01, -3.5072e-01,\n",
       "           3.0461e-01,  2.1399e-01,  1.5810e-01, -1.7082e-02, -9.4667e-02,\n",
       "          -7.4696e-02,  6.0414e-02,  2.8499e-01,  2.2221e-02, -1.1866e-01,\n",
       "          -1.1483e-01, -3.2429e-01,  2.5374e-01,  5.5751e-02,  4.2369e-01,\n",
       "          -1.5517e-01, -1.2166e-01, -3.1338e-01, -1.3274e-01, -3.8521e-02,\n",
       "          -8.2280e-02, -1.8564e-01, -4.3067e-01, -4.6088e-01, -2.4109e-01,\n",
       "          -6.7576e-02,  3.4374e-01, -1.7902e-01,  1.8433e-02,  3.6156e-01,\n",
       "           1.0913e-01,  1.6634e-01,  1.0724e-01, -8.3659e-02, -8.6611e-02,\n",
       "           1.2281e-01, -7.9654e-02, -3.1234e-01, -2.5148e-01,  1.0234e-01],\n",
       "         [ 1.2152e-01, -2.5434e-01,  1.5359e-03,  1.4021e-01,  5.7936e-02,\n",
       "          -2.9978e-01,  8.4915e-02,  1.3874e-02, -8.5623e-03, -1.5184e-02,\n",
       "           2.4282e-01,  3.7812e-01, -1.2481e-01, -1.8944e-04, -3.8465e-01,\n",
       "           1.5324e-01, -2.4513e-01, -2.0585e-01, -2.4892e-01,  2.2080e-01,\n",
       "           1.9291e-01, -1.1610e-01,  5.3531e-01, -2.1775e-01, -3.3710e-01,\n",
       "          -1.9394e-01,  1.7558e-02,  1.7000e-01, -2.7802e-01, -1.5170e-01,\n",
       "           2.1537e-01,  4.5959e-02,  2.2530e-01, -1.2241e-01,  2.2116e-01,\n",
       "           3.2271e-01,  3.7488e-01, -3.8177e-01,  4.9150e-02,  4.5160e-01,\n",
       "           2.4800e-01, -2.1036e-01, -2.4726e-01,  1.3829e-01,  2.1234e-01,\n",
       "           2.7762e-01,  1.5860e-01,  1.3931e-01,  2.9744e-02, -7.8919e-02,\n",
       "          -1.7506e-01,  4.0531e-01,  1.2331e-01, -3.7660e-01, -3.8763e-01,\n",
       "           1.7878e-01,  2.9947e-01,  1.7081e-02, -6.5856e-02,  1.7689e-01,\n",
       "          -4.8061e-02,  1.1903e-01, -1.6743e-01, -1.5054e-01,  1.7864e-01,\n",
       "           1.7809e-01,  1.9446e-01, -4.0370e-02,  2.6921e-01,  1.4642e-01,\n",
       "          -1.2777e-01,  1.3383e-01,  3.2232e-01, -8.8843e-02, -8.0283e-02,\n",
       "           2.4186e-01, -2.7853e-01, -8.2225e-02, -3.6702e-02,  6.1907e-02,\n",
       "           2.4653e-01,  3.9793e-01,  2.1220e-01, -3.2535e-01,  2.9088e-01,\n",
       "          -3.3602e-01, -1.3216e-01,  9.4619e-02,  1.0332e-01, -1.8974e-01,\n",
       "           1.4276e-01, -9.8881e-02,  3.8221e-01,  1.4198e-02, -3.2605e-02,\n",
       "          -1.7779e-01, -2.3493e-01, -1.4415e-01,  3.0294e-01, -2.4184e-01],\n",
       "         [ 1.5807e-01, -1.3919e-01, -2.4518e-01, -2.9360e-01,  6.0329e-02,\n",
       "          -1.4349e-01, -9.6500e-02,  4.4365e-01,  1.3704e-01, -3.6247e-01,\n",
       "          -2.2887e-01, -2.7873e-01, -1.5225e-01, -5.6926e-02, -1.6863e-02,\n",
       "          -2.7041e-02, -2.5792e-01,  3.9969e-01, -1.3295e-01,  3.2811e-01,\n",
       "           2.8233e-01,  2.3530e-01, -8.6704e-02,  1.6883e-01,  1.7113e-01,\n",
       "           2.4650e-01, -1.2005e-01,  4.3123e-02,  3.0854e-02,  1.4604e-01,\n",
       "           1.6162e-01, -3.1922e-02, -1.5505e-01, -2.9353e-01, -2.0193e-01,\n",
       "           3.7414e-01,  2.5059e-01, -3.9929e-01, -2.4831e-01, -4.4080e-01,\n",
       "           2.1862e-01, -4.3507e-01,  1.7970e-01,  4.9061e-01,  1.0708e-01,\n",
       "          -6.1464e-01, -3.5975e-01,  1.7931e-01,  3.2877e-01,  7.1033e-01,\n",
       "           5.1519e-01,  2.2333e-01, -1.8268e-02, -4.0543e-02, -1.1401e-01,\n",
       "          -5.0501e-01,  2.7543e-02, -3.0559e-01,  5.1533e-01,  3.0282e-01,\n",
       "          -1.5862e-01, -1.5829e-01, -2.0434e-01,  1.2981e-01, -1.5978e-01,\n",
       "           3.8879e-02, -1.5823e-02,  3.2371e-01, -1.4969e-01,  2.2409e-01,\n",
       "           4.2698e-01, -7.3731e-01, -2.8973e-01, -4.2535e-01, -2.1713e-01,\n",
       "          -1.4269e-01, -3.3192e-01, -1.5995e-01, -2.6034e-02, -2.7331e-01,\n",
       "          -1.6017e-01,  5.3479e-02, -2.8821e-02,  8.3014e-04, -4.4950e-02,\n",
       "          -8.5980e-01, -3.7816e-02,  3.6677e-02, -5.2157e-01,  2.5989e-01,\n",
       "          -4.5269e-01, -1.7978e-01,  4.5474e-01, -2.4991e-01, -1.5643e-01,\n",
       "           3.9349e-01, -1.3528e-01,  1.5577e-03,  2.1978e-01,  9.9117e-02],\n",
       "         [ 2.5321e-01,  1.9242e-01, -1.5510e-01, -7.2513e-02,  2.8637e-01,\n",
       "           2.5497e-01, -2.7463e-01, -9.8271e-02,  6.8077e-02, -1.7061e-01,\n",
       "           6.3855e-02,  3.3622e-01,  2.6581e-01, -1.0222e-01,  2.0152e-01,\n",
       "           1.6850e-01, -2.5024e-01,  1.2240e-01, -1.2883e-01,  1.4647e-01,\n",
       "          -3.3711e-03,  1.5725e-01,  1.2701e-01, -3.3027e-01, -3.1703e-01,\n",
       "           6.1790e-02, -3.9262e-01, -4.2977e-01,  2.6584e-01, -3.6032e-01,\n",
       "          -3.2456e-01,  2.2851e-01, -3.8735e-01, -1.9678e-01,  1.1802e-01,\n",
       "          -2.6224e-01, -2.3692e-01,  2.2859e-01, -2.6485e-01, -3.1533e-01,\n",
       "          -2.6130e-02,  2.6137e-01,  2.6847e-01, -1.5167e-01,  1.3831e-01,\n",
       "           1.9999e-01, -2.7218e-01,  3.1338e-01, -6.6922e-02,  1.8886e-01,\n",
       "          -2.7494e-01, -4.1565e-01,  3.1184e-01, -2.1474e-01, -4.3863e-02,\n",
       "           9.7346e-02, -2.5373e-01, -3.1278e-01,  3.7980e-01, -1.3150e-01,\n",
       "          -1.4865e-01, -2.5977e-01, -3.3692e-01,  7.8937e-02,  4.2127e-01,\n",
       "          -2.3297e-01,  2.5464e-02, -2.3311e-01, -2.8304e-01, -4.3289e-02,\n",
       "          -1.5448e-01,  1.4407e-01,  6.7188e-02,  1.4884e-01,  2.5420e-01,\n",
       "           3.9687e-01, -5.1188e-01,  1.8706e-02, -8.6669e-02, -2.7203e-01,\n",
       "           2.4242e-01,  3.2356e-01, -3.0879e-01,  6.5730e-02,  1.9708e-01,\n",
       "          -9.9230e-02, -2.2766e-01,  1.5493e-01,  3.4279e-01, -2.3940e-01,\n",
       "           1.4646e-01, -2.2507e-01,  1.7403e-01,  4.3409e-01, -1.4169e-01,\n",
       "           2.0344e-01,  5.3246e-01,  8.9019e-03,  1.2634e-01,  3.4580e-02],\n",
       "         [-7.2224e-02,  3.1136e-02, -5.1238e-02,  9.3574e-02, -3.2876e-01,\n",
       "           3.5907e-01, -1.3809e-01,  5.5996e-01,  1.6195e-01,  3.5724e-01,\n",
       "           4.8648e-01, -3.3070e-01, -3.8781e-01,  4.6618e-01,  2.8940e-01,\n",
       "          -1.1479e-01,  6.4413e-02, -1.2388e-01,  8.4395e-02,  2.4944e-01,\n",
       "           2.1436e-02, -2.3086e-01, -1.1388e-01,  1.1414e-01, -4.6255e-01,\n",
       "          -3.3265e-01,  4.8897e-01,  3.3743e-01,  2.3111e-01,  1.4780e-01,\n",
       "           3.6097e-01,  2.8574e-01,  1.5050e-01, -8.1685e-02,  9.4376e-02,\n",
       "          -3.8540e-02, -8.6795e-02,  1.7945e-01, -1.9866e-01,  1.7843e-01,\n",
       "           3.1582e-01, -1.1604e-02, -3.3643e-01,  1.5071e-01, -2.3637e-01,\n",
       "           3.8339e-01, -2.5554e-01, -8.4059e-02, -1.4492e-01, -2.1266e-01,\n",
       "          -3.2573e-01,  2.4327e-01, -4.0991e-01,  8.9642e-03,  2.3285e-01,\n",
       "          -2.4678e-01, -2.8954e-01,  2.0292e-01, -1.8543e-01,  2.3475e-01,\n",
       "           3.3820e-02,  1.3787e-02, -2.3375e-02,  1.4729e-02, -1.9821e-02,\n",
       "          -1.9167e-01, -5.1621e-01, -4.1878e-01,  2.2205e-01, -7.6606e-02,\n",
       "          -2.0111e-01,  1.4441e-01, -1.1375e-01,  2.7344e-01,  5.8827e-02,\n",
       "          -1.4141e-01,  3.8083e-01,  1.5296e-01,  4.7006e-01,  2.6611e-01,\n",
       "          -2.7268e-01,  2.7111e-01,  3.0605e-01,  9.1144e-02, -5.7035e-02,\n",
       "           2.7537e-01,  4.8006e-01,  3.3333e-01, -1.5782e-01,  2.4330e-01,\n",
       "          -5.7154e-01, -2.0885e-01, -3.5618e-01,  1.8903e-02,  4.0584e-01,\n",
       "           1.6771e-01,  7.5905e-02, -8.9794e-02,  1.8415e-01, -3.3162e-01],\n",
       "         [-7.7363e-01,  1.6377e-01,  5.2378e-02,  3.5903e-02,  1.6996e-01,\n",
       "           3.3086e-01,  2.3696e-01, -5.8980e-01,  1.0886e-01, -4.0518e-01,\n",
       "          -1.5974e-01,  3.4140e-01,  1.6131e-01,  3.1014e-01, -3.3283e-01,\n",
       "          -2.6189e-01,  2.3153e-02, -7.4064e-02, -5.3928e-02, -3.3911e-01,\n",
       "          -7.9568e-01, -1.3990e-01,  1.9336e-02, -1.9392e-01,  3.6937e-01,\n",
       "          -4.6351e-01,  1.0562e-01, -2.8038e-01, -1.4176e-01, -5.3793e-01,\n",
       "           2.3258e-01,  1.9307e-01, -9.5475e-02,  9.3109e-02,  7.6897e-02,\n",
       "          -1.0997e-01,  1.4127e-01,  1.7451e-01,  7.2986e-01,  2.3930e-02,\n",
       "           8.2826e-02,  2.2799e-01, -2.4162e-01,  1.2979e-01,  2.8641e-03,\n",
       "           7.7875e-02,  5.5455e-01,  4.5738e-01,  1.9388e-01, -3.2419e-01,\n",
       "           5.1528e-02, -5.1368e-01, -4.3605e-01, -3.9757e-01, -9.3726e-02,\n",
       "          -3.8914e-02, -1.3688e-01, -1.0116e-01, -5.7687e-01,  4.1897e-02,\n",
       "          -3.1171e-01, -3.1160e-01,  3.4089e-01, -4.3088e-03, -4.2413e-01,\n",
       "          -1.9333e-01,  8.6269e-02, -1.4820e-01, -3.5779e-01, -6.9072e-02,\n",
       "           1.5347e-01,  2.5627e-02, -2.0683e-02, -2.5528e-01, -3.8902e-01,\n",
       "           6.4160e-02,  5.6737e-01,  1.7260e-01, -2.4080e-01,  3.3277e-02,\n",
       "           1.5283e-01, -3.8327e-01, -6.6809e-02,  3.0259e-01,  1.7131e-01,\n",
       "           1.8312e-01, -3.9057e-01, -4.0706e-01,  2.8843e-01,  1.3198e-02,\n",
       "           2.6648e-01,  3.4410e-01, -2.1802e-01,  1.2194e-01, -1.3283e-01,\n",
       "           2.9480e-01, -1.4581e-03, -1.6211e-01, -3.5440e-01, -4.4964e-02],\n",
       "         [-3.1242e-01, -2.5050e-01, -7.9347e-02,  8.0039e-02, -1.8992e-01,\n",
       "          -3.1815e-01,  3.6781e-01, -2.9586e-01, -1.9390e-01,  3.0456e-01,\n",
       "          -9.5997e-02,  1.9796e-01,  4.6312e-02, -3.8940e-01,  3.3505e-01,\n",
       "          -4.3019e-01,  2.9032e-02, -1.0882e-01,  8.2107e-03, -3.4235e-01,\n",
       "          -1.3130e-01,  3.2327e-01, -2.8698e-01,  1.2776e-01, -8.5636e-03,\n",
       "           5.6175e-01,  2.7749e-01,  5.7954e-02, -4.6543e-02,  2.4464e-01,\n",
       "          -5.2822e-01, -2.0310e-01, -6.6712e-02, -1.4214e-01,  1.1201e-01,\n",
       "           2.8202e-01, -1.3285e-01, -1.6763e-01, -6.2820e-02,  3.0616e-01,\n",
       "          -3.1921e-01,  3.3042e-01,  2.4180e-01,  2.7016e-01,  3.2148e-01,\n",
       "          -3.4010e-01, -1.6814e-01, -3.1320e-01, -9.5966e-02, -1.6948e-01,\n",
       "           2.5358e-01,  1.3523e-01, -4.1037e-02,  8.8797e-03,  2.2558e-01,\n",
       "           5.0566e-02,  1.9994e-01,  3.7301e-01, -3.8841e-01,  2.0923e-01,\n",
       "           2.4714e-01,  2.2735e-01,  1.3313e-01, -3.9094e-01, -1.3967e-01,\n",
       "           2.1397e-01, -9.4659e-02,  5.0379e-02,  1.7653e-01,  2.3688e-01,\n",
       "          -2.0347e-01,  1.5779e-02,  1.2553e-01,  1.4358e-01, -2.4932e-01,\n",
       "          -1.8280e-01, -1.6618e-01,  1.7621e-01, -1.4516e-01,  3.2728e-01,\n",
       "          -2.2153e-01,  1.6473e-01,  3.6833e-02,  1.2901e-01,  4.4483e-02,\n",
       "           2.8699e-01, -3.6076e-01, -3.8570e-02,  2.4029e-01,  5.8799e-02,\n",
       "          -2.8250e-01,  1.1122e-01, -1.3252e-01, -2.0125e-01,  3.3209e-01,\n",
       "           2.2218e-01, -2.7481e-01, -1.6382e-01, -1.5001e-01, -1.2215e-01],\n",
       "         [ 1.4749e-01,  4.8784e-01,  2.2438e-02,  1.0347e-01,  1.5674e-01,\n",
       "           9.9954e-03, -4.7050e-01,  1.4788e-01,  3.9714e-01,  1.5014e-01,\n",
       "          -2.3245e-04, -3.2755e-01,  3.0861e-02, -9.4749e-02, -3.9775e-01,\n",
       "           2.7059e-01,  1.4947e-01, -1.4750e-01, -3.8402e-02, -5.3215e-02,\n",
       "           1.6821e-01, -2.6740e-01, -3.7965e-01,  2.4398e-01, -2.2323e-01,\n",
       "          -4.4742e-01, -1.1056e-01,  6.1132e-01, -2.5062e-01, -3.4312e-01,\n",
       "          -1.2496e-01, -5.2995e-01,  2.2016e-02,  4.0823e-02, -2.0627e-01,\n",
       "           1.7985e-01, -1.7979e-01,  1.9525e-01, -9.1846e-02, -3.9496e-01,\n",
       "          -8.0551e-02,  8.6591e-02, -1.3378e-01, -2.4592e-01,  4.0903e-02,\n",
       "          -2.5688e-01,  4.7983e-02,  7.8085e-02,  3.1842e-01, -1.2480e-01,\n",
       "          -3.1668e-01,  2.2160e-01, -1.1943e-01,  4.2378e-01, -2.8466e-02,\n",
       "           2.7155e-01, -1.2887e-01, -2.4659e-01, -2.8535e-02, -4.9145e-01,\n",
       "           1.1802e-01, -2.0011e-01, -4.7476e-01, -3.7456e-02,  1.2488e-02,\n",
       "          -1.5176e-01, -2.5114e-01, -1.7766e-01,  4.2463e-02, -1.3157e-01,\n",
       "          -2.8263e-01,  2.1213e-01,  1.5254e-01, -2.5745e-01, -2.0821e-01,\n",
       "           4.9325e-01, -2.3012e-01, -3.0342e-01,  3.2101e-01, -3.8625e-01,\n",
       "           2.2119e-01, -1.9049e-02,  3.9953e-01,  8.8059e-02,  6.4298e-03,\n",
       "           1.6315e-01,  1.2555e-01, -2.8884e-01, -6.4247e-01, -2.8923e-01,\n",
       "           1.5984e-01, -3.9542e-01, -1.1030e-01,  1.2245e-01, -1.5548e-01,\n",
       "          -3.5454e-01,  1.4848e-01,  4.9566e-01, -2.1885e-01,  1.9241e-01],\n",
       "         [ 3.2533e-01,  3.2352e-01, -7.2855e-02,  3.1560e-01, -2.9552e-01,\n",
       "          -2.6372e-01,  1.3223e-01,  3.6208e-01, -3.0742e-01,  1.1317e-02,\n",
       "          -6.2111e-01,  2.1682e-01, -3.2117e-02,  1.6174e-02,  6.6725e-02,\n",
       "          -3.0453e-02, -1.1591e-01,  7.4270e-02,  2.4077e-01,  4.9175e-02,\n",
       "          -5.4453e-02,  1.7206e-01,  2.3693e-01, -3.9472e-01,  4.6541e-01,\n",
       "           7.6701e-02,  1.9320e-01, -5.9408e-02,  2.6056e-01,  5.9109e-01,\n",
       "           2.1019e-01,  3.2528e-01,  3.2258e-01, -2.9890e-01, -2.8766e-01,\n",
       "          -7.4301e-02, -9.4183e-02, -7.5399e-02, -8.1019e-02,  3.0151e-01,\n",
       "           5.8981e-03, -2.2702e-01,  1.8445e-01, -3.6164e-01, -1.2887e-02,\n",
       "           2.0214e-01, -1.4236e-01,  2.8688e-01, -9.6308e-02, -2.4685e-01,\n",
       "           2.7248e-01,  2.6191e-01,  3.8414e-01,  1.3264e-01, -4.2480e-01,\n",
       "           3.7323e-02, -1.2967e-01,  2.4483e-01, -8.3828e-02,  7.8553e-02,\n",
       "           1.1170e-01, -1.9115e-01, -1.0993e-01,  2.9363e-02, -1.6341e-02,\n",
       "           2.4278e-01,  5.8713e-01,  9.1579e-02, -1.7694e-02, -1.9422e-03,\n",
       "           3.1603e-01,  3.7575e-01, -3.5745e-01,  1.4287e-01, -6.1222e-02,\n",
       "          -2.8641e-01,  3.5860e-01,  3.3433e-01,  4.4334e-03,  2.1278e-01,\n",
       "           1.5599e-01, -3.1840e-02,  3.4467e-01,  7.9518e-02, -1.4776e-01,\n",
       "           1.6208e-01,  2.4777e-01,  2.5802e-01,  2.3519e-01, -1.6146e-01,\n",
       "           1.9061e-01,  5.0592e-01, -3.3962e-01, -3.6161e-01, -2.3549e-01,\n",
       "          -2.6296e-01, -4.7797e-02,  7.4629e-02, -1.4636e-01,  3.3378e-01],\n",
       "         [ 3.3779e-01, -2.9251e-01,  4.2572e-01, -2.5700e-01,  1.5064e-01,\n",
       "           3.4919e-01,  3.2369e-01, -1.6441e-01, -3.9647e-01, -6.8118e-02,\n",
       "           4.9503e-01, -3.7914e-01, -1.3434e-01, -1.4256e-02,  2.5724e-01,\n",
       "           4.3213e-01,  1.6026e-01, -1.1529e-01, -8.9376e-02,  1.8799e-01,\n",
       "           2.1776e-01, -6.8007e-02,  1.5723e-02,  2.0316e-01,  4.1846e-01,\n",
       "           2.6541e-02, -2.1910e-01, -2.2734e-02, -6.0232e-02, -1.0002e-01,\n",
       "          -2.1613e-02,  3.0543e-02, -1.4410e-01,  5.3321e-01,  3.6009e-01,\n",
       "          -2.9612e-01, -2.1027e-01,  2.4827e-01, -1.2729e-01,  2.7618e-01,\n",
       "           1.0295e-01, -3.1930e-01,  2.5441e-01, -4.3775e-01, -2.2710e-01,\n",
       "           4.6969e-01,  2.6699e-01, -3.7061e-01, -2.4730e-01, -1.5625e-01,\n",
       "          -3.7099e-01,  4.8083e-02,  3.5012e-01, -1.7070e-01,  1.8394e-01,\n",
       "           1.2808e-01, -1.5328e-01, -9.4785e-02,  2.0721e-01,  1.7142e-01,\n",
       "          -1.4576e-01,  1.8519e-01,  6.7146e-01,  1.7904e-01,  2.6343e-01,\n",
       "          -2.6555e-01, -7.8406e-02,  2.4719e-01,  3.4333e-01, -3.4417e-01,\n",
       "          -8.7780e-02,  1.3909e-01, -2.2654e-01,  2.5625e-01,  2.9981e-01,\n",
       "          -3.2771e-01,  3.6495e-02, -1.8806e-02, -1.3151e-01,  3.0458e-01,\n",
       "          -3.6701e-01, -4.8125e-01, -1.8916e-01,  8.5217e-02, -1.3584e-01,\n",
       "           6.6476e-02,  2.3475e-01, -5.1097e-02,  2.5636e-01, -2.6569e-01,\n",
       "           2.7942e-01, -9.3178e-02, -1.4127e-01,  2.0957e-01, -2.4523e-01,\n",
       "          -5.4476e-01,  1.5772e-01,  9.5542e-02,  1.6137e-01,  3.9587e-01]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0090,  0.0056,  0.1366, -0.0656, -0.0155,  0.0458, -0.0476,  0.0843,\n",
       "         -0.1415, -0.0733], requires_grad=True)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "induced-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0080, -0.0141,  0.0383, -0.0103,  0.0013,  0.0218, -0.0356, -0.0228,\n",
       "        -0.0382, -0.0159, -0.0151,  0.0460, -0.0070,  0.0435, -0.0254, -0.0147,\n",
       "        -0.0032,  0.0105, -0.0077,  0.0004, -0.0047, -0.0055, -0.0339,  0.0173,\n",
       "        -0.0308, -0.0061,  0.0225, -0.0354, -0.0293, -0.0048, -0.0192, -0.0218,\n",
       "         0.0065,  0.0266, -0.0079, -0.0133,  0.0077,  0.0481, -0.0287,  0.0276,\n",
       "        -0.0239,  0.0289, -0.0095, -0.0055, -0.0284, -0.0274,  0.0205,  0.0408,\n",
       "         0.0140, -0.0043,  0.0287, -0.0078,  0.0090,  0.0165, -0.0393,  0.0231,\n",
       "        -0.0165, -0.0010, -0.0238,  0.0230,  0.0002, -0.0006, -0.0164,  0.0537,\n",
       "         0.0321,  0.0053, -0.0277,  0.0187,  0.0066, -0.0011,  0.0217,  0.0270,\n",
       "        -0.0283, -0.0403,  0.0196,  0.0299, -0.0023,  0.0104, -0.0132,  0.0106,\n",
       "         0.0119,  0.0370,  0.0017,  0.0263,  0.0404, -0.0017, -0.0151,  0.0179,\n",
       "        -0.0050, -0.0308,  0.0082, -0.0037, -0.0387,  0.0266,  0.0246, -0.0437,\n",
       "        -0.0268, -0.0049,  0.0263, -0.0295], requires_grad=True)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "hispanic-hybrid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0761e+00, -1.6235e+01,  2.8565e+01, -2.9188e+01,  5.7104e+01,\n",
       "          2.5931e+01,  1.0913e+02, -4.0174e+01,  3.4021e+01, -3.2643e+00,\n",
       "          1.5213e+02, -6.2860e+01, -2.4384e+01,  3.4705e+01,  1.7448e+02,\n",
       "         -7.2245e+01,  3.1114e+01, -6.9633e+01,  4.0884e+02, -2.2795e+01,\n",
       "         -5.0592e+00, -5.0908e+01, -2.6636e+01,  3.7254e+01, -7.3828e+01,\n",
       "          9.6459e+01,  6.2397e+01,  3.0369e+01,  2.6805e+01,  2.2060e+01,\n",
       "         -1.1421e+01, -5.5894e+01,  6.9189e+01,  1.8263e+01, -4.3208e+01,\n",
       "         -3.9489e+00, -3.2699e+01,  2.0531e+01,  8.4536e+00,  9.1575e+01,\n",
       "          1.6691e+01,  3.2863e+01, -1.0319e+02, -2.1753e+00, -3.5632e+01,\n",
       "         -9.1058e+01, -1.3955e+01, -3.4793e+01, -3.9361e+01,  1.4621e+01,\n",
       "          6.0975e+01,  1.9006e+01,  4.7710e+01, -4.9742e+01,  1.2436e+01,\n",
       "          1.6198e+01, -7.0281e+00,  2.7315e+01,  2.5393e+01, -5.8661e+01,\n",
       "          1.7992e+01,  2.6399e+02,  5.6909e+00, -1.4564e+01,  2.7287e+01,\n",
       "         -2.3878e+01,  1.9201e+02,  3.7534e+01, -1.4834e+02, -1.7137e+01,\n",
       "          5.9946e+01, -3.2439e+01,  2.1900e+01, -1.0830e+01,  5.5036e+01,\n",
       "          2.6049e+01,  4.2267e+01,  8.9683e+01, -2.9262e+01, -4.2697e+01,\n",
       "         -2.0576e+01, -2.7347e+01, -2.2184e+01,  4.3418e+01,  3.4864e+01,\n",
       "         -9.5323e+01, -5.8547e+00,  2.2754e+02, -2.3670e+01,  2.2805e+01,\n",
       "         -8.9867e+01,  3.1030e+01, -5.2088e+00,  4.5674e+01, -1.5165e+02,\n",
       "         -1.7612e+01, -5.3979e+01, -1.2877e+02, -3.7780e+02,  4.2549e+01],\n",
       "        [ 6.3888e+01,  6.3495e+00,  2.1252e+01, -1.7166e+02, -2.1891e+01,\n",
       "         -1.0906e+02, -6.9815e+01, -8.9468e+01,  2.1744e+01, -1.7615e+01,\n",
       "         -6.4894e+01, -5.8981e+01,  2.5228e+01,  9.5426e+00, -9.1179e+01,\n",
       "          1.0559e+02, -1.3284e+02,  8.3737e+00, -1.5286e+02,  1.8807e+02,\n",
       "          7.7380e+01,  2.2341e+02,  1.0428e+02, -3.4822e+02,  1.1122e+02,\n",
       "         -1.5712e+02, -5.2405e+00, -3.6342e+01, -2.5043e+02,  5.7740e+00,\n",
       "          1.9851e+01,  1.6995e+02,  9.8667e+01,  4.2622e+01,  4.2894e+01,\n",
       "          1.7359e+02,  5.8633e+01,  5.9647e+00, -3.6040e+01, -2.3356e+02,\n",
       "          1.0764e+02, -6.9488e+01,  4.9572e+01,  9.7755e+00, -1.3095e+02,\n",
       "         -1.6551e+01, -1.1087e+01, -6.9502e+01, -4.4153e+01, -1.4967e+02,\n",
       "         -6.6506e+01, -2.6192e+02,  3.4045e+01, -5.5812e+01,  1.3992e+02,\n",
       "          3.3856e+00,  5.4832e+01, -5.4881e+01,  6.3696e+01, -1.7658e+01,\n",
       "          1.2622e+01, -1.1612e+02,  5.3447e+01, -2.3043e+01, -1.3549e+01,\n",
       "          3.5023e+00, -2.1186e+02, -1.9198e+01,  5.9682e+01, -2.5030e+01,\n",
       "          1.6369e+02,  1.5582e+02,  5.9436e+01,  7.5980e+00,  4.5135e+01,\n",
       "         -1.3084e+02,  7.0311e+01, -8.2911e+01,  1.4543e+01, -6.5215e+01,\n",
       "         -9.9081e+00,  2.9498e+01, -1.1631e+00, -6.6350e+01, -3.7190e+01,\n",
       "          1.1965e+02, -3.1738e+01, -2.3251e+01, -6.9955e+01,  7.4448e+01,\n",
       "          1.9038e+02,  5.4964e+01,  4.4082e+02,  2.6534e+01,  2.1227e+02,\n",
       "         -2.3474e+01,  2.5090e+01,  2.9427e+02,  4.7510e+02, -4.5985e+01],\n",
       "        [-3.4220e+01, -3.7699e+01, -5.7732e+01,  1.0453e+02,  1.3914e+01,\n",
       "         -1.6358e+01, -6.2125e+00, -1.9574e+01, -3.0001e+01,  3.6697e+00,\n",
       "          1.0901e+02,  2.3302e+01,  2.1052e+01,  1.7131e+00, -1.7042e+02,\n",
       "          7.4555e+01, -5.3177e+01,  6.8059e+01, -7.5604e+01, -1.7762e+01,\n",
       "          5.4652e+01, -2.5319e+01, -4.4725e+01, -3.8783e+01,  5.3560e+01,\n",
       "         -4.4991e+01,  1.1717e+02, -3.9205e+01, -6.6223e+01,  3.5675e+01,\n",
       "          7.7170e+01, -5.3963e+01, -1.1177e+02, -5.3996e+01,  6.7670e+00,\n",
       "         -1.7888e+02, -5.3951e+00, -2.2224e+01, -1.8310e+01,  4.5177e+01,\n",
       "         -5.4914e+01, -1.0899e+02,  5.6115e+01,  4.0709e+00, -3.6717e+01,\n",
       "         -3.2330e+01, -2.3227e+02, -3.9899e+01,  1.1356e+01, -7.8504e+01,\n",
       "          3.5307e+00,  2.0757e+01, -1.4149e+01,  4.6780e+01, -7.0604e+00,\n",
       "         -3.3002e+01, -3.1054e+01, -3.7885e+01, -8.8581e+01, -1.4397e+01,\n",
       "          1.3218e+01, -3.5052e+02, -3.2412e+01,  8.6499e+01, -6.6372e+00,\n",
       "          1.0550e+01, -6.1640e+01, -2.3846e+01, -7.2797e+00, -4.6906e+01,\n",
       "         -9.0025e+01,  4.1073e+01, -2.8510e+01, -2.4671e+01,  1.2466e+01,\n",
       "         -5.8403e+01,  2.5882e+01, -1.6184e+00, -1.0066e+01, -6.7929e+01,\n",
       "          5.0504e+01, -1.4959e+02, -3.7107e+01, -3.6962e+01,  2.5165e+01,\n",
       "          1.5717e+02,  8.4822e+00, -3.9669e+01,  8.1629e+01, -5.9214e+01,\n",
       "          1.4810e+01,  7.5851e+00, -2.1190e+02, -1.7189e+01,  1.0698e+02,\n",
       "          2.3015e+01, -2.0704e+01,  1.2337e+02, -5.7971e+01,  4.4582e+00],\n",
       "        [-5.0778e+01, -7.4182e-01, -3.7135e+01,  3.6605e+02, -5.7145e+00,\n",
       "         -3.4650e-01,  1.8894e+02, -5.6267e+01, -8.9250e+00, -2.7623e+00,\n",
       "         -3.7977e+01, -7.3947e+00,  4.3530e+01, -6.6719e+01, -2.8163e+02,\n",
       "          5.8093e+01, -5.0363e+01,  1.1635e+01, -7.6646e+01,  7.0693e+00,\n",
       "         -7.5766e+01, -1.9236e+02,  2.2575e+02, -1.8890e+00, -7.9903e+01,\n",
       "          2.3887e+01, -7.7867e+01, -1.6021e+01, -3.1259e+01,  9.3821e+01,\n",
       "          5.2822e-01, -1.2558e+02, -1.4052e+02, -6.0892e+01,  1.2331e+01,\n",
       "         -3.5102e+01,  3.1422e+01, -2.9365e+00, -1.2948e+02, -1.3205e+01,\n",
       "         -3.3355e+01,  1.7578e+02, -9.7432e+01, -1.1810e+00,  9.6353e+01,\n",
       "          8.3897e+01, -5.1150e+01,  5.2938e+01,  8.6894e+00,  9.2210e+01,\n",
       "         -9.4253e+01, -9.4298e+00, -3.5137e+01,  9.6806e+00,  1.4129e+02,\n",
       "         -1.4163e+01, -7.3406e+00,  3.5931e+01, -2.4163e+01, -3.4291e+01,\n",
       "          5.5937e+01,  2.2646e+01, -4.9363e+01,  1.1753e+02, -6.6994e+00,\n",
       "          2.1486e+01,  2.8363e+01, -6.4503e+01,  1.3484e+01, -3.7949e+01,\n",
       "          1.8237e+01,  1.9902e+02, -1.7252e+01, -6.0405e-01, -3.5023e+01,\n",
       "         -3.9571e+01, -6.1559e+00,  4.7006e+01,  1.0131e+02,  2.3032e+02,\n",
       "          5.7837e+01, -1.8862e+01, -8.4107e+01, -2.9870e+01, -1.9860e+00,\n",
       "         -3.9954e+01,  5.8156e+01, -6.8922e+01, -4.4219e+00, -5.6065e+01,\n",
       "         -2.6442e+02, -1.2251e+02, -2.6255e+02, -1.7522e+01,  1.1676e+01,\n",
       "          1.5043e+01, -2.1070e+01,  1.9190e+01, -7.0813e+02, -2.8127e+01],\n",
       "        [-2.5535e+01,  2.0265e+01, -6.7111e+00,  6.6088e+01, -5.5596e+00,\n",
       "         -6.2219e+01,  2.0301e+02,  9.0076e+01, -1.7478e+01,  3.3862e+01,\n",
       "          7.7134e+01,  3.3944e+01,  6.3991e+01,  2.3816e+01,  3.6220e+02,\n",
       "         -1.0129e+02,  2.4040e+01, -6.9800e+00, -2.4122e+01, -8.2320e+00,\n",
       "         -3.5083e+01, -9.6968e+01,  1.1179e+01,  1.6117e+02, -1.4863e+02,\n",
       "          3.0843e+02, -1.4114e+02,  3.2358e+01,  1.5118e+02, -1.0729e+01,\n",
       "          6.6430e+00,  1.3181e+02, -4.2576e+01, -4.3289e+00,  7.5739e+01,\n",
       "          1.0926e+02,  2.4085e+01, -9.0395e+01,  3.6458e+01, -6.1171e+01,\n",
       "         -1.6690e+00, -3.9484e+00,  6.3227e+01,  4.5796e+01,  3.3820e+01,\n",
       "          3.2077e+01, -8.3561e+00, -3.5004e+01, -1.5283e+00, -6.2018e+00,\n",
       "          1.0074e+02,  1.5344e+01, -1.3185e+01,  6.3865e+01,  2.2393e+01,\n",
       "         -7.2494e+00,  3.0037e+00, -7.9148e+01, -1.6653e+01,  6.8241e+01,\n",
       "         -3.1050e+00, -1.5560e+02, -8.2378e+00, -6.8988e+01, -9.3982e+00,\n",
       "          3.8884e+01,  1.6767e+02,  2.2579e+01, -1.0022e+01,  6.0288e+01,\n",
       "          7.4336e+01, -4.0114e+02, -2.1448e+01, -9.7468e+00,  7.7567e+00,\n",
       "          2.9926e+02, -9.7810e+00, -6.9960e+01, -2.9021e+01, -4.2611e+01,\n",
       "          2.5588e+00, -3.1714e+01,  3.2647e+01,  3.8914e+01, -1.4411e+01,\n",
       "         -8.6132e+00,  4.1886e+00,  3.5667e+01,  6.3555e+01, -1.0386e+02,\n",
       "         -2.4826e+01,  5.9633e+01,  1.2473e+02,  2.0734e+01, -2.0207e+02,\n",
       "          2.5957e+01,  2.7337e+01, -1.5676e+02,  2.6141e+02,  2.4360e+01],\n",
       "        [ 2.9210e+01, -3.6676e+01, -5.3098e+01,  7.6697e+01,  5.8587e+00,\n",
       "          1.3667e+02, -6.3358e+01,  8.0159e+01,  2.7155e+01, -4.3537e+01,\n",
       "         -9.7158e+01,  7.4936e+01, -1.4341e+01, -6.8773e+01,  2.1541e+02,\n",
       "         -5.0007e+01,  8.6279e+01,  1.2458e+01,  5.0964e+01, -9.3237e+01,\n",
       "         -4.9030e+01,  3.0045e+01, -1.0050e+02, -7.8866e+01, -4.0250e+01,\n",
       "          1.1714e+02,  6.5622e+01, -1.4546e+01,  2.1496e+02,  1.3880e+01,\n",
       "          3.4922e+01, -1.9270e+01, -1.4914e+01, -7.0882e+01, -4.8140e+01,\n",
       "          6.3804e+01, -2.8944e+01,  1.1510e+01,  4.7261e+01,  1.2079e+02,\n",
       "          1.0079e+01,  2.1557e+02,  1.0084e+02, -1.5562e+01, -2.6485e+01,\n",
       "          1.1308e+02,  2.3150e+02,  1.3660e+02, -2.9825e+01,  3.9079e+01,\n",
       "          3.9333e-01, -3.3042e+01,  7.4442e+00, -2.0862e+01, -3.3876e+01,\n",
       "         -4.4275e+00,  2.4363e+00,  1.0887e+02,  2.7500e+01, -1.5945e+02,\n",
       "         -2.1705e+01, -1.6578e+02, -5.8431e+01, -1.8203e+01,  1.1256e+01,\n",
       "          3.9262e+00, -1.2489e+02,  1.7529e+01, -8.3084e+01, -5.9436e+01,\n",
       "         -4.2896e+01, -1.5660e+02,  1.1945e+01, -7.8350e+01, -3.1100e+00,\n",
       "         -2.4119e+02, -5.9631e+01, -1.9558e+01,  4.3334e+01,  6.2011e-01,\n",
       "          9.2894e+01,  1.1710e+02,  2.3975e+01,  2.5923e+01,  1.3320e+01,\n",
       "          6.8645e+01, -8.2031e+00,  3.1945e+00,  5.5067e+01,  2.4994e+01,\n",
       "         -1.6263e+02, -6.4736e+01, -4.8295e+02, -2.0660e+01, -1.5519e+02,\n",
       "         -9.2851e+00, -2.7780e+01,  1.2126e+01, -4.1787e+02, -2.7977e+01],\n",
       "        [ 2.7863e+01,  2.2864e+01,  6.3849e+01, -9.9894e+01, -2.3058e+01,\n",
       "          5.5624e+00, -6.4720e+02, -4.8270e+01, -1.2479e+00,  7.3261e+01,\n",
       "         -6.7624e+01, -1.2569e+00, -5.2944e+00,  3.0946e+01, -1.0867e+02,\n",
       "         -1.2183e+01,  3.2934e+01,  3.9827e+01,  2.1948e+02, -6.1233e+01,\n",
       "          9.4816e+01, -2.7325e+02, -4.8528e+00, -1.4158e+02, -1.2698e+01,\n",
       "          4.5209e+01,  4.3736e+01, -2.5714e+00, -9.2553e+01, -5.2938e+01,\n",
       "         -2.6358e+00, -3.5126e+01,  1.1965e+02,  3.1748e+01,  2.9714e+01,\n",
       "          4.7223e+01, -1.8658e+01,  4.0158e+00, -2.0873e+01, -6.5914e+01,\n",
       "         -1.0635e+02, -7.1674e+01,  5.7714e+01, -4.8951e+01, -9.8687e+01,\n",
       "         -3.9815e+01, -3.0554e+01, -3.7988e+01, -7.4145e+01,  7.6452e+01,\n",
       "         -2.0404e+01,  2.0197e+01,  4.0731e+01, -5.1776e+01, -5.6612e+01,\n",
       "          1.1324e+01,  2.4573e+01, -2.9683e+00,  1.7808e+01,  1.0606e+02,\n",
       "         -2.0612e+01,  1.4390e+02,  1.1781e+01, -5.4597e+00, -7.8493e+01,\n",
       "         -4.8871e+01,  1.1273e+02, -1.4681e+01,  1.0980e+02, -3.6514e+01,\n",
       "          8.0194e+01, -3.1198e+02,  3.1567e+00,  8.5530e-01, -4.6745e+01,\n",
       "          1.0935e+02, -1.7095e+01, -3.5322e+01, -2.1161e+01, -5.0420e+01,\n",
       "         -4.2382e+01, -1.7545e+00,  2.2361e+01, -5.9894e+01, -4.0153e+01,\n",
       "         -1.3668e+02, -3.8899e+01, -1.3194e+01, -8.8683e+01,  1.6288e+02,\n",
       "         -8.2228e+01, -1.4837e+01,  5.6913e+01,  4.2480e+00,  1.5479e+02,\n",
       "         -9.1203e-01,  3.1805e+01, -3.3433e+01, -1.5193e+02,  1.4178e+01],\n",
       "        [ 3.1927e+01,  2.6076e+01,  6.0864e+01, -2.4906e+02,  5.3527e+01,\n",
       "         -1.4094e+02,  9.7482e+01, -2.4309e+01,  5.1093e+01, -2.8225e+01,\n",
       "         -1.4387e+02,  6.5206e+01, -1.2013e+02,  7.3158e+01, -2.0612e+02,\n",
       "          1.1763e+02, -5.2661e+01,  1.4611e+00, -6.2254e+01,  2.8061e+01,\n",
       "          8.2907e+01,  5.3934e+02, -7.3829e+00,  2.2726e+02,  2.4032e+02,\n",
       "         -3.9669e+02,  1.2650e+02,  6.1322e+01,  2.2581e+02, -2.2636e+01,\n",
       "         -7.2559e+01, -4.5642e+01, -9.1114e+01,  6.7632e+01, -5.5111e+01,\n",
       "         -1.5723e+02, -1.7334e+01,  7.5460e+01,  6.4072e+01, -1.6234e+00,\n",
       "          6.9578e+01, -1.2433e+01,  4.0621e+01,  2.3498e+01,  1.4240e+02,\n",
       "          1.1543e+01,  1.6394e+01,  1.0625e+02,  1.8354e+02,  7.2195e+01,\n",
       "          1.1633e+01,  1.2836e+02, -3.6649e+01,  8.4071e+01, -1.3009e+02,\n",
       "          5.0896e+01,  1.5211e+01,  7.4114e+01,  5.9386e+01, -4.0883e+01,\n",
       "         -3.6431e+01,  7.7065e+02,  7.1953e+01,  3.3199e+01,  4.0194e+01,\n",
       "         -2.6461e+01,  1.0480e+02,  3.2594e+00, -1.2233e+00,  1.1319e+02,\n",
       "         -4.2702e+02,  1.9134e+02,  5.6362e+01,  9.1620e+01,  5.8367e+01,\n",
       "          5.2028e+01,  1.3382e+00,  3.3395e+01, -3.7561e+01,  1.4877e+02,\n",
       "         -7.9048e+01,  2.7080e+02,  5.7581e+01,  1.0284e+02, -6.8330e+01,\n",
       "          6.9865e+01,  8.4434e+00, -1.0466e+02,  9.9882e+01, -8.1678e+01,\n",
       "          2.2209e+02,  1.0428e+02, -3.4601e+02,  4.5327e+01,  2.3558e+02,\n",
       "         -1.7105e+01,  2.7033e+01, -8.7805e+00,  4.0410e+02,  2.5414e+00],\n",
       "        [-4.6857e+01,  1.1070e+01, -5.4472e+01,  1.4139e+02, -5.8597e-01,\n",
       "          6.2197e+01,  4.0382e+01,  5.2502e+01, -4.0948e+01,  1.0061e+01,\n",
       "          3.7299e+01, -6.8863e+01,  1.3843e+01, -4.0720e+01,  7.2646e+01,\n",
       "          1.5460e+01, -2.6373e+00, -1.5407e+01, -1.8616e+02,  3.3165e+01,\n",
       "          1.1206e+01, -1.1937e+02, -5.2528e+01, -7.8909e+01,  4.9762e+00,\n",
       "         -1.2550e+02, -6.1169e+01, -3.9397e+01, -2.2690e+02, -5.5673e+00,\n",
       "         -8.7850e+00, -5.7457e+01,  1.2443e+02, -2.0237e+01, -2.8859e+01,\n",
       "          1.9810e+01, -1.7775e+00, -3.4580e+01,  6.5421e+00,  9.8773e+01,\n",
       "         -1.4179e+01, -1.9605e+02, -1.2847e+02,  1.6437e+01, -1.2075e+01,\n",
       "         -1.6837e+02, -5.3302e+01, -2.1411e+01, -1.1955e+01, -7.8119e+01,\n",
       "          4.8964e+01,  8.1824e+01, -1.6698e+01,  5.5346e+01, -6.2077e+01,\n",
       "         -1.4223e+01, -6.1207e+01,  6.6723e+01, -3.1369e+01,  3.3526e+01,\n",
       "          1.2336e+01, -5.3795e+02, -5.0964e+01,  8.3360e+00,  1.1107e+01,\n",
       "          1.8716e+01, -7.1307e+01, -1.9298e+01,  3.9718e+01,  1.0255e+01,\n",
       "          2.2503e+02, -1.1416e+01, -5.5747e+01,  1.4390e+01, -1.1392e+01,\n",
       "         -1.0184e+02,  1.5086e+01,  3.8071e+01, -1.6810e+01, -6.2215e+01,\n",
       "          6.2608e+01, -2.1135e+02, -3.0761e+01,  3.6153e+01,  6.7828e+01,\n",
       "          2.7767e+01,  3.2499e+01, -5.8890e+01,  4.2295e+01,  7.9086e+01,\n",
       "          7.4040e+01, -3.2243e+01,  2.9001e+02, -3.7268e+01, -2.7624e+02,\n",
       "          3.1283e+01, -2.1365e+01, -5.7023e+01, -5.5668e+01,  7.8170e+01],\n",
       "        [ 2.2808e+00,  4.6552e+00,  3.4368e+01, -2.0481e+02, -7.3324e+01,\n",
       "          9.8368e+01,  1.4756e+02,  5.5407e+01, -3.5252e+01, -2.5871e+01,\n",
       "          3.5976e+01,  2.0728e+00, -3.4548e+00,  2.5057e+00,  3.3191e+01,\n",
       "         -1.3553e+02,  1.1704e+02, -4.9714e+01, -1.0146e+02, -5.3299e+01,\n",
       "         -1.5616e+02, -3.4880e+01, -1.0439e+02,  2.6254e+02, -5.5023e+01,\n",
       "          1.3303e+02, -1.2960e+02,  2.4157e+01,  4.8199e+01, -7.9351e+01,\n",
       "         -4.3561e+01,  9.0925e+01, -1.1250e+01,  5.0253e+01,  7.8743e+00,\n",
       "         -3.8587e+01, -9.6752e+00,  3.2989e+01,  4.1991e+01,  1.9351e+01,\n",
       "          6.5633e+00,  3.8339e+01, -3.9143e+01, -3.1659e+01,  6.8063e+01,\n",
       "          1.0766e+02,  1.5266e+02, -5.7241e+01, -2.3694e+00,  1.8003e+01,\n",
       "         -4.4906e+01,  1.9024e+01, -1.3860e+01, -8.1581e+01, -2.6465e+01,\n",
       "         -8.7480e+00,  6.5163e+00, -1.3824e+02, -3.2847e+01,  1.1753e+02,\n",
       "         -3.0555e+01,  1.2480e+02,  5.6736e+01, -1.1484e+02,  2.5270e+01,\n",
       "          2.3392e+00, -1.3622e+02,  6.0843e+01,  2.7669e+01,  3.8941e+01,\n",
       "         -6.1347e+01,  3.2650e+02, -3.0078e+01,  9.7084e+00, -8.2675e+01,\n",
       "          8.5100e+01, -6.2265e+01,  1.6565e+00, -1.5335e+01, -4.8959e+01,\n",
       "         -1.1466e+02,  2.3275e+01,  3.8761e+01, -5.4039e+01,  2.0668e+01,\n",
       "         -1.6236e+02, -2.6817e+01,  4.1949e+01, -1.5556e+02, -6.3575e+01,\n",
       "          1.2246e+02, -2.3412e+01,  3.9641e+02, -4.9961e+01,  6.3982e+01,\n",
       "         -2.6826e+01,  3.3763e+01, -6.4339e+01,  6.2861e+02, -6.4338e+01]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
