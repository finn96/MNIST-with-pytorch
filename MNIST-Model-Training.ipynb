{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels).long()\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')\n",
    "print(train_labels[z][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)  # stride = 1\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782*2,100) # 80\n",
    "        #self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = F.max_pool1d(F.sigmoid(self.conv1(x)), (2))  ## TODO: Test this again against whatever you leave here\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        #x = x.view(-1, x.shape[1:].numel())\n",
    "        #x = self.drop1(x)\n",
    "        x = x.view(-1, x.shape[1:].numel())  # Linear input should be in shape [batch_size, features x height x width]\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        \n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        #x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        # What we're doing here is re-shaping x to remove the channel dimension. This was confusing for me to understand. But\n",
    "        # basically pytorch expects everything in mini batches. So incoming data is in the form (images_per_batch,\n",
    "        # channels_per_image, (h,w)). In our case, we have (4,1,784), because we have 4 images per batch, 1 channel per image,\n",
    "        # and a flat vector of length 784 representing that channel (and because we have only one channel, that image). Our\n",
    "        # output, however, is channel indepedent. We want our output to be in the shape (images_per_batch,\n",
    "        # {single_image_output_shape}). In that case, our final output shape is (4, 10)\n",
    "        return self.fc2(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%\n",
    "#https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + pytorch bias, Sigmoid, Kernel Size 3: 85%, 86%, 86%\n",
    "# 5 Epochs with pytorch weights/bias, Sigmoid, Kernel Size 3: 87%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + 0 bias, Sigmoid, Kernel Size 3: 86%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights and bias, Sigmoid, Kernel Size 3: 86%, 85%, 86%\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 3: 87%, 87%, 86%,(5) -- 90.35% 90.44%  (10) **\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 5: 86%, 86%, 86% (5) -- 89.31% 90.03%  (10)\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 8: 87%, 86%, 87% (5) -- 89.79% 88.58%  (10)\n",
    "\n",
    "## TODO: !DF! Best of the normalized Xavier head-to-head with pytorch defaults, 10 epochs, track loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "# TODO:!DF! test with and without these weights\n",
    "#net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc1.bias.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ =  net.fc2.bias.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))-np.sqrt(6.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training Run: 4, Epoch: 4, Batch: 58000, running_loss: 57.149008901556954'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "criterion = nn.MSELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "training_outs = []\n",
    "for training_run in range(5):\n",
    "    net = Net()\n",
    "    net.fc1.bias.data.fill_(0)\n",
    "    net.fc2.bias.data.fill_(0)\n",
    "    net.fc1.weight.data.uniform_(-np.sqrt(6.0)/np.sqrt(782+100),np.sqrt(6.0)/np.sqrt(782+100))\n",
    "    _ = net.fc2.weight.data.uniform_(-np.sqrt(6.0)/np.sqrt(100+10),np.sqrt(6.0)/np.sqrt(100+10))\n",
    "    for epoch in range(5):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0,len(train_images), 4):\n",
    "            net.zero_grad()\n",
    "            out = net.forward(train_images[i:i+4])\n",
    "            #loss = criterion(out, train_labels[i:i+4])\n",
    "            loss = criterion2(out, t_labels[i:i+4])\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 0:\n",
    "                ## I recommend doing things like this. It helped me catch mistakes\n",
    "                #print(batch[0][0])\n",
    "                #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(targets[0])\n",
    "                clear_output(wait=True)\n",
    "                display(f\"Training Run: {training_run}, Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "                running_loss = 0.0\n",
    "            for f in net.parameters():\n",
    "                f.data.sub_(f.grad.data * 0.01)\n",
    "    correct = []\n",
    "    for i,img in enumerate(test_imgs):\n",
    "        res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "        targ = test_labels[i]\n",
    "        if res == targ:\n",
    "            correct.append(1)\n",
    "        else:\n",
    "            correct.append(0)\n",
    "    training_outs.append(sum(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "motivated-difficulty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9469.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9485.4"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "#default_wb_results = copy.deepcopy(training_outs)\n",
    "norm_xav_wb_results = copy.deepcopy(training_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "signal-bryan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.0296, 20.4730, 19.2990, 18.5638, 17.8782, 17.0853, 16.1400, 15.1221,\n",
       "        14.1541, 13.2988])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.flatten().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "fitted-lingerie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1536,  0.2882, -0.4127, -0.0651,  0.3566,  0.1120, -0.2545,  0.0706,\n",
       "         -0.7031, -0.1304],\n",
       "        [ 0.1242,  0.2807, -0.4189, -0.0767,  0.3486,  0.1419, -0.2370,  0.0854,\n",
       "         -0.6636, -0.1226],\n",
       "        [ 0.1348,  0.2714, -0.3890, -0.0232,  0.3746,  0.1413, -0.2490,  0.0728,\n",
       "         -0.6497, -0.1143],\n",
       "        [ 0.0769,  0.2961, -0.3989, -0.0508,  0.3697,  0.1184, -0.2663,  0.0687,\n",
       "         -0.6492, -0.1373]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9376 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = t_test_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1050,  0.4207,  0.8344,  8.9818, -2.0588,  2.4454, -7.6334, -3.6497,\n",
       "         2.2114, -0.3338], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9],\n",
       "        [3],\n",
       "        [8],\n",
       "        [2]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stone-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0344, -0.0227, -0.0099,  ...,  0.0357,  0.0121, -0.0129],\n",
       "        [-0.0289, -0.0260, -0.0018,  ...,  0.0205, -0.0217,  0.0199],\n",
       "        [ 0.0091,  0.0305,  0.0191,  ..., -0.0006,  0.0047, -0.0305],\n",
       "        ...,\n",
       "        [-0.0057,  0.0190, -0.0134,  ..., -0.0067,  0.0096, -0.0161],\n",
       "        [ 0.0202, -0.0128, -0.0338,  ..., -0.0176, -0.0176,  0.0211],\n",
       "        [-0.0263, -0.0035,  0.0092,  ..., -0.0211, -0.0109, -0.0111]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(782,100)\n",
    "ll2 = nn.Linear(782, 100)\n",
    "ll2.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "requested-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6307e-05)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "neither-marking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.7258e-05)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll2.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "southern-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "useful-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "threaded-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "christian-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[0][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "temporal-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "structural-return",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "injured-ceremony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "authorized-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_labels.size()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-advance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
