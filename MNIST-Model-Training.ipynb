{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels).long()\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#_ = train_images[:][:].div_(255.0)  # Normalize the pixel values to between [0,1.0]\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,  18.,  18.,\n",
      "        126., 136., 175.,  26., 166., 255., 247., 127.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94., 154.,\n",
      "        170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,  64.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49.,\n",
      "        238., 253., 253., 253., 253., 253., 253., 253., 253., 251.,  93.,  82.,\n",
      "         82.,  56.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,  18., 219., 253., 253., 253., 253., 253., 198., 182.,\n",
      "        247., 241.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
      "        253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0., 139., 253., 190.,   2.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11.,\n",
      "        190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240., 253.,\n",
      "        253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0., 249., 253., 249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253., 250., 182.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,  24., 114., 221., 253., 253., 253.,\n",
      "        253., 201.,  78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213., 253.,\n",
      "        253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171.,\n",
      "        219., 253., 253., 253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         55., 172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,  16.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd670a5d310>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANeElEQVR4nO3df6hc9ZnH8c9nzQ/RJpAoxmjs2i0RdwmYSgyrkdVVW1wRY/+oNkFwWeEWSSARYQ3dYIWwEFa7qyAWb1WaXbophcQ1BNlWQlyDYEki2Zg02+jGbHKbX2j+aKpCTPLsH/dErvHOmZuZc+ZM7vN+wWVmzjPnnIfRT86Z852ZryNCAMa/P2m6AQC9QdiBJAg7kARhB5Ig7EASE3q5M9tc+gdqFhEebXlXR3bbd9v+ne0PbK/oZlsA6uVOx9ltXyRpr6RvSxqStFXSooj4bck6HNmBmtVxZJ8v6YOI2BcRJyX9QtLCLrYHoEbdhP1qSQdHPB4qln2J7QHb22xv62JfALrUzQW60U4VvnKaHhGDkgYlTuOBJnVzZB+SdM2Ix7MkHequHQB16SbsWyXNtv0N25MkfV/ShmraAlC1jk/jI+KU7aWSfiXpIkmvRMTuyjpDJebMmVNa37lzZ2n9ueeeK60/9thj590TmtHVh2oi4nVJr1fUC4Aa8XFZIAnCDiRB2IEkCDuQBGEHkiDsQBI9/T47+g+/LpwHR3YgCcIOJEHYgSQIO5AEYQeSIOxAEgy9jQMTJ05sWVu1alUPO0E/48gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzj4OTJ48uWXtvvvu62En6Gcc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZx4Err7yytm0fOXKktm2jt7oKu+39kk5IOi3pVETMq6IpANWr4sj+1xHxUQXbAVAj3rMDSXQb9pD0a9vbbQ+M9gTbA7a32d7W5b4AdKHb0/gFEXHI9hWS3rD9PxHx1sgnRMSgpEFJss3EYkBDujqyR8Sh4vaYpFclza+iKQDV6zjsti+1PeXsfUnfkbSrqsYAVKub0/gZkl61fXY7/x4R/1lJVzgvK1eurG3be/furW3b6K2Owx4R+yTdUGEvAGrE0BuQBGEHkiDsQBKEHUiCsANJ8BXXC8CDDz5YWn/ooYc63vbGjRtL6xs2bOh42+gvHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2S8Ay5cvL60XXzPuyLPPPltaP336dMfbRn/hyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDO3gfuvffe0vr111/fo04wnnFkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvA4sXLy6tT506tbR+6tSplrVVq1aVrvvOO++U1jF+tD2y237F9jHbu0Ysm277DdvvF7fT6m0TQLfGchr/M0l3n7NshaRNETFb0qbiMYA+1jbsEfGWpOPnLF4oaU1xf42k+6ttC0DVOn3PPiMiDktSRBy2fUWrJ9oekDTQ4X4AVKT2C3QRMShpUJJsR937AzC6TofejtqeKUnF7bHqWgJQh07DvkHSw8X9hyW9Vk07AOriiPIza9trJd0u6XJJRyX9SNJ/SPqlpK9LOiDpexFx7kW80baV8jR+ypQppfWtW7eW1q+77rrS+ieffNLxvjH+RMSoEwm0fc8eEYtalO7sqiMAPcXHZYEkCDuQBGEHkiDsQBKEHUiCr7j2wOOPP15anz17dmm93fDo22+/fd49jQdz5swprc+YMaNlbcuWLaXrnjx5sqOe+hlHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Iou1XXCvdWdKvuJ44caK0fskll5TW9+3bV1q/8cYbO953t9p9huCGG25oWZs7d27pup9++mknLX3hpptualm76667StfdvHlzV/tuUquvuHJkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk+D57BebNm1danzRpUmm9bMplSXrmmWdK63WOpS9ZsqS0/vTTT5fWe/k5jvOxfv360vpVV11VWv/ss8+qbKcnOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs1eg3feyJ0wof5lfe618evsXX3zxfFuqzKxZs2rb9hNPPFFaHxoa6mr7l112Wcva6tWrS9ddvHhxaf3ll1/uqKcmtT2y237F9jHbu0Yse8r2723vKP7uqbdNAN0ay2n8zyTdPcryf4mIucXf69W2BaBqbcMeEW9JOt6DXgDUqJsLdEtt7yxO86e1epLtAdvbbG/rYl8AutRp2H8i6ZuS5ko6LOnHrZ4YEYMRMS8iyr8tAqBWHYU9Io5GxOmIOCPpp5LmV9sWgKp1FHbbM0c8/K6kXa2eC6A/tB1nt71W0u2SLrc9JOlHkm63PVdSSNov6Qf1tTj+HTx4sLReNl4sSR9//HGV7XzJAw880NX6L730UsvaCy+8ULpuu9+NX7BgQWn9lltuaVlr91v941HbsEfEolEWX3ifKACS4+OyQBKEHUiCsANJEHYgCcIOJMFXXCuwY8eO0nq7n4peunRpaX3q1Kkdr3/y5MnSdT///PPSerfuuOOOlrVly5aVrls2dCZJd955Z2l98uTJpfUy7f6bXYg4sgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEu7llLq2+3P+3ppt3769tN7up6jbKRsTbrfv3bt3l9bbfcW13WcAzpw5U1rvxocfflhaX7duXctau74fffTRjnrqBxHh0ZZzZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhn74FJkyaV1p9//vnS+iOPPFJlO5WyRx3S/ULZ/19vvvlm6brz55fPPfLkk0+W1teuXduyduTIkdJ1L2SMswPJEXYgCcIOJEHYgSQIO5AEYQeSIOxAEoyz94GLL764tH7bbbeV1leuXNmy1u6317vVbpz9wIEDLWs333xz6boTJpRPa9BuquusOh5nt32N7c2299jebXtZsXy67Tdsv1/cTqu6aQDVGctp/ClJj0fEn0v6S0lLbP+FpBWSNkXEbEmbiscA+lTbsEfE4Yh4t7h/QtIeSVdLWihpTfG0NZLur6lHABU4r7nebF8r6VuSfiNpRkQclob/QbB9RYt1BiQNdNkngC6NOey2vyZpnaTlEfGHdhdmzoqIQUmDxTa4QAc0ZExDb7YnajjoP4+I9cXio7ZnFvWZko7V0yKAKrQdevPwIXyNpOMRsXzE8qclfRwRq22vkDQ9Iv6+zbY4sgM1azX0Npaw3yppi6T3JJ39EfAfavh9+y8lfV3SAUnfi4jjbbZF2IGadRz2KhF2oH78eAWQHGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJtA277Wtsb7a9x/Zu28uK5U/Z/r3tHcXfPfW3C6BTY5mffaakmRHxru0pkrZLul/SA5L+GBHPjHlnTNkM1K7VlM0TxrDiYUmHi/snbO+RdHW17QGo23m9Z7d9raRvSfpNsWip7Z22X7E9rcU6A7a32d7WXasAutH2NP6LJ9pfk/Rfkv4xItbbniHpI0khaZWGT/X/rs02OI0HatbqNH5MYbc9UdJGSb+KiH8epX6tpI0RMafNdgg7ULNWYR/L1XhLelnSnpFBLy7cnfVdSbu6bRJAfcZyNf5WSVskvSfpTLH4h5IWSZqr4dP4/ZJ+UFzMK9sWR3agZl2dxleFsAP16/g0HsD4QNiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii7Q9OVuwjSf834vHlxbJ+1K+99WtfEr11qsre/rRVoaffZ//Kzu1tETGvsQZK9Gtv/dqXRG+d6lVvnMYDSRB2IImmwz7Y8P7L9Gtv/dqXRG+d6klvjb5nB9A7TR/ZAfQIYQeSaCTstu+2/TvbH9he0UQPrdjeb/u9YhrqRuenK+bQO2Z714hl022/Yfv94nbUOfYa6q0vpvEumWa80deu6enPe/6e3fZFkvZK+rakIUlbJS2KiN/2tJEWbO+XNC8iGv8Ahu2/kvRHSf96dmot2/8k6XhErC7+oZwWEU/0SW9P6Tyn8a6pt1bTjP+tGnztqpz+vBNNHNnnS/ogIvZFxElJv5C0sIE++l5EvCXp+DmLF0paU9xfo+H/WXquRW99ISIOR8S7xf0Tks5OM97oa1fSV080EfarJR0c8XhI/TXfe0j6te3ttgeabmYUM85Os1XcXtFwP+dqO413L50zzXjfvHadTH/erSbCPtrUNP00/rcgIm6U9DeSlhSnqxibn0j6pobnADws6cdNNlNMM75O0vKI+EOTvYw0Sl89ed2aCPuQpGtGPJ4l6VADfYwqIg4Vt8ckvarhtx395OjZGXSL22MN9/OFiDgaEacj4oykn6rB166YZnydpJ9HxPpiceOv3Wh99ep1ayLsWyXNtv0N25MkfV/Shgb6+ArblxYXTmT7UknfUf9NRb1B0sPF/YclvdZgL1/SL9N4t5pmXA2/do1Pfx4RPf+TdI+Gr8j/r6R/aKKHFn39maT/Lv52N92bpLUaPq37XMNnRI9IukzSJknvF7fT+6i3f9Pw1N47NRysmQ31dquG3xrulLSj+Lun6deupK+evG58XBZIgk/QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAS/w/WgTKDcHO3lwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "print(train_images[0][0])\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)  # stride = 1\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782,100) # 80\n",
    "        #self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        #x = F.sigmoid(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = x.view(-1, x.shape[1:].numel())  # Linear input should be in shape [batch_size, features x height x width]\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        \n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        #x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        # What we're doing here is re-shaping x to remove the channel dimension. This was confusing for me to understand. But\n",
    "        # basically pytorch expects everything in mini batches. So incoming data is in the form (images_per_batch,\n",
    "        # channels_per_image, (h,w)). In our case, we have (4,1,784), because we have 4 images per batch, 1 channel per image,\n",
    "        # and a flat vector of length 784 representing that channel (and because we have only one channel, that image). Our\n",
    "        # output, however, is channel indepedent. We want our output to be in the shape (images_per_batch,\n",
    "        # {single_image_output_shape}). In that case, our final output shape is (4, 10)\n",
    "        return self.fc2(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%\n",
    "#https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + pytorch bias, Sigmoid, Kernel Size 3: 85%, 86%, 86%\n",
    "# 5 Epochs with pytorch weights/bias, Sigmoid, Kernel Size 3: 87%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + 0 bias, Sigmoid, Kernel Size 3: 86%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights and bias, Sigmoid, Kernel Size 3: 86%, 85%, 86%\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 3: 87%, 87%, 86%,(5) -- 90.35% 90.44%  (10) **\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 5: 86%, 86%, 86% (5) -- 89.31% 90.03%  (10)\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 8: 87%, 86%, 87% (5) -- 89.79% 88.58%  (10)\n",
    "\n",
    "## TODO: !DF! Best of the normalized Xavier head-to-head with pytorch defaults, 10 epochs, track loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "# TODO:!DF! test with and without these weights\n",
    "#net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc1.bias.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ =  net.fc2.bias.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))-np.sqrt(6.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch: 2, Batch: 14999, running_loss: 74.57530490626232'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "criterion = nn.MSELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "mb_size = 4\n",
    "num_batches = len(train_images) // mb_size\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    #for i in range(0,len(train_images), 4):\n",
    "    for i in range(0, num_batches):\n",
    "        net.zero_grad()\n",
    "        out = net.forward(train_images[i*mb_size:(i+1)*mb_size])\n",
    "            #loss = criterion(out, train_labels[i:i+4])\n",
    "        loss = criterion2(out, t_labels[i*mb_size:(i+1)*mb_size])\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "                ## I recommend doing things like this. It helped me catch mistakes\n",
    "                #print(batch[0][0])\n",
    "                #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(targets[0])\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * 0.01)\n",
    "            #f.data.sub_(f.grad.data * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "motivated-difficulty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9454.6\n",
      "9480.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "default_wb_results = copy.deepcopy(training_outs)\n",
    "#norm_xav_wb_results = copy.deepcopy(training_outs)\n",
    "print(np.mean(norm_xav_wb_results))\n",
    "print(np.mean(default_wb_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "endangered-yellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1536,  0.2882, -0.4127, -0.0651,  0.3566,  0.1120, -0.2545,  0.0706,\n",
       "         -0.7031, -0.1304],\n",
       "        [ 0.1242,  0.2807, -0.4189, -0.0767,  0.3486,  0.1419, -0.2370,  0.0854,\n",
       "         -0.6636, -0.1226],\n",
       "        [ 0.1348,  0.2714, -0.3890, -0.0232,  0.3746,  0.1413, -0.2490,  0.0728,\n",
       "         -0.6497, -0.1143],\n",
       "        [ 0.0769,  0.2961, -0.3989, -0.0508,  0.3697,  0.1184, -0.2663,  0.0687,\n",
       "         -0.6492, -0.1373]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9614 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-91ac2a9860c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9496\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "img = t_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1050,  0.4207,  0.8344,  8.9818, -2.0588,  2.4454, -7.6334, -3.6497,\n",
       "         2.2114, -0.3338], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9],\n",
       "        [3],\n",
       "        [8],\n",
       "        [2]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stone-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0344, -0.0227, -0.0099,  ...,  0.0357,  0.0121, -0.0129],\n",
       "        [-0.0289, -0.0260, -0.0018,  ...,  0.0205, -0.0217,  0.0199],\n",
       "        [ 0.0091,  0.0305,  0.0191,  ..., -0.0006,  0.0047, -0.0305],\n",
       "        ...,\n",
       "        [-0.0057,  0.0190, -0.0134,  ..., -0.0067,  0.0096, -0.0161],\n",
       "        [ 0.0202, -0.0128, -0.0338,  ..., -0.0176, -0.0176,  0.0211],\n",
       "        [-0.0263, -0.0035,  0.0092,  ..., -0.0211, -0.0109, -0.0111]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(782,100)\n",
    "ll2 = nn.Linear(782, 100)\n",
    "ll2.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "requested-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6307e-05)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "neither-marking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.7258e-05)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll2.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "southern-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "useful-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "threaded-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "christian-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[0][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "temporal-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tested-heavy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "chronic-prison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "found-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_labels.size()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "simple-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class enumerate in module builtins:\n",
      "\n",
      "class enumerate(object)\n",
      " |  enumerate(iterable, start=0)\n",
      " |  \n",
      " |  Return an enumerate object.\n",
      " |  \n",
      " |    iterable\n",
      " |      an object supporting iteration\n",
      " |  \n",
      " |  The enumerate object yields pairs containing a count (from start, which\n",
      " |  defaults to zero) and a value yielded by the iterable argument.\n",
      " |  \n",
      " |  enumerate is useful for obtaining an indexed list:\n",
      " |      (0, seq[0]), (1, seq[1]), (2, seq[2]), ...\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(enumerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-overview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
