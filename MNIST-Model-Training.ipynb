{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels)\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')\n",
    "print(train_labels[z][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782,100) # 80\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = F.max_pool1d(F.sigmoid(self.conv1(x)), (2))  ## TODO: Test this again against whatever you leave here\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        #x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        return self.fc2(x)\n",
    "        #return self.fc3(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# TODO:!DF! test with and without these weights\n",
    "net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc2.bias.data\n",
    "#net.fc3.weight.data.uniform_(-1.0/np.sqrt(80),1.0/np.sqrt(80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, running_loss: 0.3842152953147888\n",
      "Epoch: 0, Batch: 2000, running_loss: 46.38825993984938\n",
      "Epoch: 0, Batch: 4000, running_loss: 43.24665227532387\n",
      "Epoch: 0, Batch: 6000, running_loss: 41.554694175720215\n",
      "Epoch: 0, Batch: 8000, running_loss: 40.372930347919464\n",
      "Epoch: 0, Batch: 10000, running_loss: 38.94594490155578\n",
      "Epoch: 0, Batch: 12000, running_loss: 37.53005349636078\n",
      "Epoch: 0, Batch: 14000, running_loss: 36.75769066810608\n",
      "Epoch: 0, Batch: 16000, running_loss: 35.99681656435132\n",
      "Epoch: 0, Batch: 18000, running_loss: 34.7306363992393\n",
      "Epoch: 0, Batch: 20000, running_loss: 33.106588777154684\n",
      "Epoch: 0, Batch: 22000, running_loss: 32.05949483066797\n",
      "Epoch: 0, Batch: 24000, running_loss: 31.757390279322863\n",
      "Epoch: 0, Batch: 26000, running_loss: 30.47219653427601\n",
      "Epoch: 0, Batch: 28000, running_loss: 30.163912538439035\n",
      "Epoch: 0, Batch: 30000, running_loss: 29.34362237341702\n",
      "Epoch: 0, Batch: 32000, running_loss: 29.73763306811452\n",
      "Epoch: 0, Batch: 34000, running_loss: 28.517619635909796\n",
      "Epoch: 0, Batch: 36000, running_loss: 27.80790983699262\n",
      "Epoch: 0, Batch: 38000, running_loss: 27.49714821577072\n",
      "Epoch: 0, Batch: 40000, running_loss: 26.828467721119523\n",
      "Epoch: 0, Batch: 42000, running_loss: 26.430908327922225\n",
      "Epoch: 0, Batch: 44000, running_loss: 26.48491637222469\n",
      "Epoch: 0, Batch: 46000, running_loss: 26.047132413834333\n",
      "Epoch: 0, Batch: 48000, running_loss: 25.421235166490078\n",
      "Epoch: 0, Batch: 50000, running_loss: 25.56764150597155\n",
      "Epoch: 0, Batch: 52000, running_loss: 24.754015215672553\n",
      "Epoch: 0, Batch: 54000, running_loss: 24.459720104932785\n",
      "Epoch: 0, Batch: 56000, running_loss: 23.577547937631607\n",
      "Epoch: 0, Batch: 58000, running_loss: 23.489051890559494\n",
      "Epoch: 1, Batch: 0, running_loss: 0.0440567247569561\n",
      "Epoch: 1, Batch: 2000, running_loss: 24.000316329300404\n",
      "Epoch: 1, Batch: 4000, running_loss: 22.92788280826062\n",
      "Epoch: 1, Batch: 6000, running_loss: 23.115077761933208\n",
      "Epoch: 1, Batch: 8000, running_loss: 23.24103847052902\n",
      "Epoch: 1, Batch: 10000, running_loss: 23.01121700834483\n",
      "Epoch: 1, Batch: 12000, running_loss: 22.876157882623374\n",
      "Epoch: 1, Batch: 14000, running_loss: 23.705841915681958\n",
      "Epoch: 1, Batch: 16000, running_loss: 24.237724708393216\n",
      "Epoch: 1, Batch: 18000, running_loss: 23.53271456900984\n",
      "Epoch: 1, Batch: 20000, running_loss: 21.751218783669174\n",
      "Epoch: 1, Batch: 22000, running_loss: 22.17014600802213\n",
      "Epoch: 1, Batch: 24000, running_loss: 22.958073291927576\n",
      "Epoch: 1, Batch: 26000, running_loss: 22.108025518245995\n",
      "Epoch: 1, Batch: 28000, running_loss: 22.62481747288257\n",
      "Epoch: 1, Batch: 30000, running_loss: 22.358063420280814\n",
      "Epoch: 1, Batch: 32000, running_loss: 23.515787839889526\n",
      "Epoch: 1, Batch: 34000, running_loss: 22.358754047192633\n",
      "Epoch: 1, Batch: 36000, running_loss: 21.86031783465296\n",
      "Epoch: 1, Batch: 38000, running_loss: 22.456242490559816\n",
      "Epoch: 1, Batch: 40000, running_loss: 21.867185031063855\n",
      "Epoch: 1, Batch: 42000, running_loss: 21.694789106026292\n",
      "Epoch: 1, Batch: 44000, running_loss: 22.058564400300384\n",
      "Epoch: 1, Batch: 46000, running_loss: 22.07545119896531\n",
      "Epoch: 1, Batch: 48000, running_loss: 21.575908759608865\n",
      "Epoch: 1, Batch: 50000, running_loss: 22.025986009277403\n",
      "Epoch: 1, Batch: 52000, running_loss: 20.9997552158311\n",
      "Epoch: 1, Batch: 54000, running_loss: 21.016745117492974\n",
      "Epoch: 1, Batch: 56000, running_loss: 20.12753319554031\n",
      "Epoch: 1, Batch: 58000, running_loss: 20.280075932852924\n",
      "Epoch: 2, Batch: 0, running_loss: 0.03588630259037018\n",
      "Epoch: 2, Batch: 2000, running_loss: 21.25795274693519\n",
      "Epoch: 2, Batch: 4000, running_loss: 19.947839168831706\n",
      "Epoch: 2, Batch: 6000, running_loss: 20.565555135719478\n",
      "Epoch: 2, Batch: 8000, running_loss: 20.77540309354663\n",
      "Epoch: 2, Batch: 10000, running_loss: 20.896003486122936\n",
      "Epoch: 2, Batch: 12000, running_loss: 20.589116596616805\n",
      "Epoch: 2, Batch: 14000, running_loss: 21.581300710327923\n",
      "Epoch: 2, Batch: 16000, running_loss: 22.386829497292638\n",
      "Epoch: 2, Batch: 18000, running_loss: 21.46484287781641\n",
      "Epoch: 2, Batch: 20000, running_loss: 19.627930633723736\n",
      "Epoch: 2, Batch: 22000, running_loss: 20.192215147428215\n",
      "Epoch: 2, Batch: 24000, running_loss: 21.310492783784866\n",
      "Epoch: 2, Batch: 26000, running_loss: 20.488851111382246\n",
      "Epoch: 2, Batch: 28000, running_loss: 20.91997470986098\n",
      "Epoch: 2, Batch: 30000, running_loss: 20.750024852342904\n",
      "Epoch: 2, Batch: 32000, running_loss: 21.87753641232848\n",
      "Epoch: 2, Batch: 34000, running_loss: 20.697641723789275\n",
      "Epoch: 2, Batch: 36000, running_loss: 20.35222602635622\n",
      "Epoch: 2, Batch: 38000, running_loss: 21.245150060392916\n",
      "Epoch: 2, Batch: 40000, running_loss: 20.394743798300624\n",
      "Epoch: 2, Batch: 42000, running_loss: 20.309756291098893\n",
      "Epoch: 2, Batch: 44000, running_loss: 20.74419761635363\n",
      "Epoch: 2, Batch: 46000, running_loss: 20.80778111424297\n",
      "Epoch: 2, Batch: 48000, running_loss: 20.382825242355466\n",
      "Epoch: 2, Batch: 50000, running_loss: 20.83028988353908\n",
      "Epoch: 2, Batch: 52000, running_loss: 19.690512413159013\n",
      "Epoch: 2, Batch: 54000, running_loss: 19.877563094720244\n",
      "Epoch: 2, Batch: 56000, running_loss: 18.997376940213144\n",
      "Epoch: 2, Batch: 58000, running_loss: 19.131759921088815\n",
      "Epoch: 3, Batch: 0, running_loss: 0.03379411622881889\n",
      "Epoch: 3, Batch: 2000, running_loss: 20.24855584371835\n",
      "Epoch: 3, Batch: 4000, running_loss: 18.79277126165107\n",
      "Epoch: 3, Batch: 6000, running_loss: 19.571202792227268\n",
      "Epoch: 3, Batch: 8000, running_loss: 19.87720798049122\n",
      "Epoch: 3, Batch: 10000, running_loss: 20.034939208999276\n",
      "Epoch: 3, Batch: 12000, running_loss: 19.657236168161035\n",
      "Epoch: 3, Batch: 14000, running_loss: 20.66826461441815\n",
      "Epoch: 3, Batch: 16000, running_loss: 21.564390970394015\n",
      "Epoch: 3, Batch: 18000, running_loss: 20.59950474044308\n",
      "Epoch: 3, Batch: 20000, running_loss: 18.71879888419062\n",
      "Epoch: 3, Batch: 22000, running_loss: 19.25750987417996\n",
      "Epoch: 3, Batch: 24000, running_loss: 20.553562271408737\n",
      "Epoch: 3, Batch: 26000, running_loss: 19.676746560027823\n",
      "Epoch: 3, Batch: 28000, running_loss: 20.073734094388783\n",
      "Epoch: 3, Batch: 30000, running_loss: 19.95632575172931\n",
      "Epoch: 3, Batch: 32000, running_loss: 21.074858563952148\n",
      "Epoch: 3, Batch: 34000, running_loss: 19.86229035910219\n",
      "Epoch: 3, Batch: 36000, running_loss: 19.592995795421302\n",
      "Epoch: 3, Batch: 38000, running_loss: 20.605558287352324\n",
      "Epoch: 3, Batch: 40000, running_loss: 19.670324977487326\n",
      "Epoch: 3, Batch: 42000, running_loss: 19.573434459976852\n",
      "Epoch: 3, Batch: 44000, running_loss: 20.06602722592652\n",
      "Epoch: 3, Batch: 46000, running_loss: 20.14262174628675\n",
      "Epoch: 3, Batch: 48000, running_loss: 19.692342039197683\n",
      "Epoch: 3, Batch: 50000, running_loss: 20.14280253648758\n",
      "Epoch: 3, Batch: 52000, running_loss: 18.94396568555385\n",
      "Epoch: 3, Batch: 54000, running_loss: 19.199743703007698\n",
      "Epoch: 3, Batch: 56000, running_loss: 18.35526727233082\n",
      "Epoch: 3, Batch: 58000, running_loss: 18.474325806833804\n",
      "Epoch: 4, Batch: 0, running_loss: 0.03209583833813667\n",
      "Epoch: 4, Batch: 2000, running_loss: 19.59012852795422\n",
      "Epoch: 4, Batch: 4000, running_loss: 18.08578820992261\n",
      "Epoch: 4, Batch: 6000, running_loss: 18.91178655344993\n",
      "Epoch: 4, Batch: 8000, running_loss: 19.279255555942655\n",
      "Epoch: 4, Batch: 10000, running_loss: 19.45537389907986\n",
      "Epoch: 4, Batch: 12000, running_loss: 19.041230064816773\n",
      "Epoch: 4, Batch: 14000, running_loss: 20.03757844120264\n",
      "Epoch: 4, Batch: 16000, running_loss: 20.941164933145046\n",
      "Epoch: 4, Batch: 18000, running_loss: 19.971391717903316\n",
      "Epoch: 4, Batch: 20000, running_loss: 18.11839896440506\n",
      "Epoch: 4, Batch: 22000, running_loss: 18.617284489795566\n",
      "Epoch: 4, Batch: 24000, running_loss: 19.957916816696525\n",
      "Epoch: 4, Batch: 26000, running_loss: 19.09584301477298\n",
      "Epoch: 4, Batch: 28000, running_loss: 19.441952493507415\n",
      "Epoch: 4, Batch: 30000, running_loss: 19.356348368804902\n",
      "Epoch: 4, Batch: 32000, running_loss: 20.417794231325388\n",
      "Epoch: 4, Batch: 34000, running_loss: 19.20830062404275\n",
      "Epoch: 4, Batch: 36000, running_loss: 18.965670872479677\n",
      "Epoch: 4, Batch: 38000, running_loss: 20.052530135959387\n",
      "Epoch: 4, Batch: 40000, running_loss: 19.08586011826992\n",
      "Epoch: 4, Batch: 42000, running_loss: 18.971358657814562\n",
      "Epoch: 4, Batch: 44000, running_loss: 19.517124840058386\n",
      "Epoch: 4, Batch: 46000, running_loss: 19.563715858384967\n",
      "Epoch: 4, Batch: 48000, running_loss: 19.103092678822577\n",
      "Epoch: 4, Batch: 50000, running_loss: 19.548751269467175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Batch: 52000, running_loss: 18.332820639014244\n",
      "Epoch: 4, Batch: 54000, running_loss: 18.623434809036553\n",
      "Epoch: 4, Batch: 56000, running_loss: 17.78602289687842\n",
      "Epoch: 4, Batch: 58000, running_loss: 17.89673050865531\n",
      "Epoch: 5, Batch: 0, running_loss: 0.030313873663544655\n",
      "Epoch: 5, Batch: 2000, running_loss: 18.981735589914024\n",
      "Epoch: 5, Batch: 4000, running_loss: 17.458728382829577\n",
      "Epoch: 5, Batch: 6000, running_loss: 18.296514101792127\n",
      "Epoch: 5, Batch: 8000, running_loss: 18.69200872629881\n",
      "Epoch: 5, Batch: 10000, running_loss: 18.863675351720303\n",
      "Epoch: 5, Batch: 12000, running_loss: 18.441970754880458\n",
      "Epoch: 5, Batch: 14000, running_loss: 19.432088705711067\n",
      "Epoch: 5, Batch: 16000, running_loss: 20.328678723424673\n",
      "Epoch: 5, Batch: 18000, running_loss: 19.362796218367293\n",
      "Epoch: 5, Batch: 20000, running_loss: 17.50675371987745\n",
      "Epoch: 5, Batch: 22000, running_loss: 18.013050688896328\n",
      "Epoch: 5, Batch: 24000, running_loss: 19.339076307602227\n",
      "Epoch: 5, Batch: 26000, running_loss: 18.51215892098844\n",
      "Epoch: 5, Batch: 28000, running_loss: 18.797803946305066\n",
      "Epoch: 5, Batch: 30000, running_loss: 18.749078910797834\n",
      "Epoch: 5, Batch: 32000, running_loss: 19.777592883445323\n",
      "Epoch: 5, Batch: 34000, running_loss: 18.54970616661012\n",
      "Epoch: 5, Batch: 36000, running_loss: 18.3087100898847\n",
      "Epoch: 5, Batch: 38000, running_loss: 19.4442892447114\n",
      "Epoch: 5, Batch: 40000, running_loss: 18.452983603812754\n",
      "Epoch: 5, Batch: 42000, running_loss: 18.326274913735688\n",
      "Epoch: 5, Batch: 44000, running_loss: 18.910747239366174\n",
      "Epoch: 5, Batch: 46000, running_loss: 18.916402235627174\n",
      "Epoch: 5, Batch: 48000, running_loss: 18.46470560366288\n",
      "Epoch: 5, Batch: 50000, running_loss: 18.888752563856542\n",
      "Epoch: 5, Batch: 52000, running_loss: 17.678544219583273\n",
      "Epoch: 5, Batch: 54000, running_loss: 17.980402268934995\n",
      "Epoch: 5, Batch: 56000, running_loss: 17.14289129152894\n",
      "Epoch: 5, Batch: 58000, running_loss: 17.223954875953496\n",
      "Epoch: 6, Batch: 0, running_loss: 0.028220409527420998\n",
      "Epoch: 6, Batch: 2000, running_loss: 18.29279733262956\n",
      "Epoch: 6, Batch: 4000, running_loss: 16.766689892392606\n",
      "Epoch: 6, Batch: 6000, running_loss: 17.602102611213923\n",
      "Epoch: 6, Batch: 8000, running_loss: 18.004053256940097\n",
      "Epoch: 6, Batch: 10000, running_loss: 18.176964576821774\n",
      "Epoch: 6, Batch: 12000, running_loss: 17.741475249640644\n",
      "Epoch: 6, Batch: 14000, running_loss: 18.72402965463698\n",
      "Epoch: 6, Batch: 16000, running_loss: 19.591025647707283\n",
      "Epoch: 6, Batch: 18000, running_loss: 18.65385964559391\n",
      "Epoch: 6, Batch: 20000, running_loss: 16.769446954131126\n",
      "Epoch: 6, Batch: 22000, running_loss: 17.31023593619466\n",
      "Epoch: 6, Batch: 24000, running_loss: 18.591250931844115\n",
      "Epoch: 6, Batch: 26000, running_loss: 17.807408831315115\n",
      "Epoch: 6, Batch: 28000, running_loss: 18.028293676674366\n",
      "Epoch: 6, Batch: 30000, running_loss: 18.023467607796192\n",
      "Epoch: 6, Batch: 32000, running_loss: 19.015761019662023\n",
      "Epoch: 6, Batch: 34000, running_loss: 17.76066039223224\n",
      "Epoch: 6, Batch: 36000, running_loss: 17.514417223632336\n",
      "Epoch: 6, Batch: 38000, running_loss: 18.661784795112908\n",
      "Epoch: 6, Batch: 40000, running_loss: 17.684361178893596\n",
      "Epoch: 6, Batch: 42000, running_loss: 17.545973259024322\n",
      "Epoch: 6, Batch: 44000, running_loss: 18.135136150754988\n",
      "Epoch: 6, Batch: 46000, running_loss: 18.08339694328606\n",
      "Epoch: 6, Batch: 48000, running_loss: 17.680971403140575\n",
      "Epoch: 6, Batch: 50000, running_loss: 18.06874493462965\n",
      "Epoch: 6, Batch: 52000, running_loss: 16.875902445986867\n",
      "Epoch: 6, Batch: 54000, running_loss: 17.17811935674399\n",
      "Epoch: 6, Batch: 56000, running_loss: 16.328480736352503\n",
      "Epoch: 6, Batch: 58000, running_loss: 16.353303669486195\n",
      "Epoch: 7, Batch: 0, running_loss: 0.026020649820566177\n",
      "Epoch: 7, Batch: 2000, running_loss: 17.432263975497335\n",
      "Epoch: 7, Batch: 4000, running_loss: 15.922170247416943\n",
      "Epoch: 7, Batch: 6000, running_loss: 16.745531260501593\n",
      "Epoch: 7, Batch: 8000, running_loss: 17.14129842631519\n",
      "Epoch: 7, Batch: 10000, running_loss: 17.318669614382088\n",
      "Epoch: 7, Batch: 12000, running_loss: 16.864280222915113\n",
      "Epoch: 7, Batch: 14000, running_loss: 17.836368639953434\n",
      "Epoch: 7, Batch: 16000, running_loss: 18.666903007775545\n",
      "Epoch: 7, Batch: 18000, running_loss: 17.764062884496525\n",
      "Epoch: 7, Batch: 20000, running_loss: 15.847068134695292\n",
      "Epoch: 7, Batch: 22000, running_loss: 16.440120546147227\n",
      "Epoch: 7, Batch: 24000, running_loss: 17.6591140544042\n",
      "Epoch: 7, Batch: 26000, running_loss: 16.92768191592768\n",
      "Epoch: 7, Batch: 28000, running_loss: 17.06529329251498\n",
      "Epoch: 7, Batch: 30000, running_loss: 17.109597926493734\n",
      "Epoch: 7, Batch: 32000, running_loss: 18.085344777442515\n",
      "Epoch: 7, Batch: 34000, running_loss: 16.797648936975747\n",
      "Epoch: 7, Batch: 36000, running_loss: 16.543312368914485\n",
      "Epoch: 7, Batch: 38000, running_loss: 17.670495231170207\n",
      "Epoch: 7, Batch: 40000, running_loss: 16.741396699566394\n",
      "Epoch: 7, Batch: 42000, running_loss: 16.589932600036263\n",
      "Epoch: 7, Batch: 44000, running_loss: 17.167986751999706\n",
      "Epoch: 7, Batch: 46000, running_loss: 17.03989377943799\n",
      "Epoch: 7, Batch: 48000, running_loss: 16.726936655584723\n",
      "Epoch: 7, Batch: 50000, running_loss: 17.046490668784827\n",
      "Epoch: 7, Batch: 52000, running_loss: 15.866635150276124\n",
      "Epoch: 7, Batch: 54000, running_loss: 16.182036254554987\n",
      "Epoch: 7, Batch: 56000, running_loss: 15.325362900272012\n",
      "Epoch: 7, Batch: 58000, running_loss: 15.317121935077012\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "for epoch in range(8):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0,len(train_images), 4):\n",
    "        net.zero_grad()\n",
    "        out = net(train_images[i:i+4])\n",
    "        loss = criterion(out, train_labels[i:i+4])\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:\n",
    "            ## I recommend doing things like this. It helped me catch mistakes\n",
    "            #print(batch[0][0])\n",
    "            #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "            #plt.show()\n",
    "            #print(targets[0])\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * 0.01)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t_images[750], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8766 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = t_test_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-portfolio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
