{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels)\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')\n",
    "print(train_labels[z][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782,100) # 80\n",
    "        self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = F.max_pool1d(F.sigmoid(self.conv1(x)), (2))  ## TODO: Test this again against whatever you leave here\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        #x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        return self.fc2(x)\n",
    "        #return self.fc3(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# TODO:!DF! test with and without these weights\n",
    "net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc2.bias.data\n",
    "#net.fc3.weight.data.uniform_(-1.0/np.sqrt(80),1.0/np.sqrt(80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, running_loss: 0.3842152953147888\n",
      "Epoch: 0, Batch: 2000, running_loss: 46.38825993984938\n",
      "Epoch: 0, Batch: 4000, running_loss: 43.24665227532387\n",
      "Epoch: 0, Batch: 6000, running_loss: 41.554694175720215\n",
      "Epoch: 0, Batch: 8000, running_loss: 40.372930347919464\n",
      "Epoch: 0, Batch: 10000, running_loss: 38.94594490155578\n",
      "Epoch: 0, Batch: 12000, running_loss: 37.53005349636078\n",
      "Epoch: 0, Batch: 14000, running_loss: 36.75769066810608\n",
      "Epoch: 0, Batch: 16000, running_loss: 35.99681656435132\n",
      "Epoch: 0, Batch: 18000, running_loss: 34.7306363992393\n",
      "Epoch: 0, Batch: 20000, running_loss: 33.106588777154684\n",
      "Epoch: 0, Batch: 22000, running_loss: 32.05949483066797\n",
      "Epoch: 0, Batch: 24000, running_loss: 31.757390279322863\n",
      "Epoch: 0, Batch: 26000, running_loss: 30.47219653427601\n",
      "Epoch: 0, Batch: 28000, running_loss: 30.163912538439035\n",
      "Epoch: 0, Batch: 30000, running_loss: 29.34362237341702\n",
      "Epoch: 0, Batch: 32000, running_loss: 29.73763306811452\n",
      "Epoch: 0, Batch: 34000, running_loss: 28.517619635909796\n",
      "Epoch: 0, Batch: 36000, running_loss: 27.80790983699262\n",
      "Epoch: 0, Batch: 38000, running_loss: 27.49714821577072\n",
      "Epoch: 0, Batch: 40000, running_loss: 26.828467721119523\n",
      "Epoch: 0, Batch: 42000, running_loss: 26.430908327922225\n",
      "Epoch: 0, Batch: 44000, running_loss: 26.48491637222469\n",
      "Epoch: 0, Batch: 46000, running_loss: 26.047132413834333\n",
      "Epoch: 0, Batch: 48000, running_loss: 25.421235166490078\n",
      "Epoch: 0, Batch: 50000, running_loss: 25.56764150597155\n",
      "Epoch: 0, Batch: 52000, running_loss: 24.754015215672553\n",
      "Epoch: 0, Batch: 54000, running_loss: 24.459720104932785\n",
      "Epoch: 0, Batch: 56000, running_loss: 23.577547937631607\n",
      "Epoch: 0, Batch: 58000, running_loss: 23.489051890559494\n",
      "Epoch: 1, Batch: 0, running_loss: 0.0440567247569561\n",
      "Epoch: 1, Batch: 2000, running_loss: 24.000316329300404\n",
      "Epoch: 1, Batch: 4000, running_loss: 22.92788280826062\n",
      "Epoch: 1, Batch: 6000, running_loss: 23.115077761933208\n",
      "Epoch: 1, Batch: 8000, running_loss: 23.24103847052902\n",
      "Epoch: 1, Batch: 10000, running_loss: 23.01121700834483\n",
      "Epoch: 1, Batch: 12000, running_loss: 22.876157882623374\n",
      "Epoch: 1, Batch: 14000, running_loss: 23.705841915681958\n",
      "Epoch: 1, Batch: 16000, running_loss: 24.237724708393216\n",
      "Epoch: 1, Batch: 18000, running_loss: 23.53271456900984\n",
      "Epoch: 1, Batch: 20000, running_loss: 21.751218783669174\n",
      "Epoch: 1, Batch: 22000, running_loss: 22.17014600802213\n",
      "Epoch: 1, Batch: 24000, running_loss: 22.958073291927576\n",
      "Epoch: 1, Batch: 26000, running_loss: 22.108025518245995\n",
      "Epoch: 1, Batch: 28000, running_loss: 22.62481747288257\n",
      "Epoch: 1, Batch: 30000, running_loss: 22.358063420280814\n",
      "Epoch: 1, Batch: 32000, running_loss: 23.515787839889526\n",
      "Epoch: 1, Batch: 34000, running_loss: 22.358754047192633\n",
      "Epoch: 1, Batch: 36000, running_loss: 21.86031783465296\n",
      "Epoch: 1, Batch: 38000, running_loss: 22.456242490559816\n",
      "Epoch: 1, Batch: 40000, running_loss: 21.867185031063855\n",
      "Epoch: 1, Batch: 42000, running_loss: 21.694789106026292\n",
      "Epoch: 1, Batch: 44000, running_loss: 22.058564400300384\n",
      "Epoch: 1, Batch: 46000, running_loss: 22.07545119896531\n",
      "Epoch: 1, Batch: 48000, running_loss: 21.575908759608865\n",
      "Epoch: 1, Batch: 50000, running_loss: 22.025986009277403\n",
      "Epoch: 1, Batch: 52000, running_loss: 20.9997552158311\n",
      "Epoch: 1, Batch: 54000, running_loss: 21.016745117492974\n",
      "Epoch: 1, Batch: 56000, running_loss: 20.12753319554031\n",
      "Epoch: 1, Batch: 58000, running_loss: 20.280075932852924\n",
      "Epoch: 2, Batch: 0, running_loss: 0.03588630259037018\n",
      "Epoch: 2, Batch: 2000, running_loss: 21.25795274693519\n",
      "Epoch: 2, Batch: 4000, running_loss: 19.947839168831706\n",
      "Epoch: 2, Batch: 6000, running_loss: 20.565555135719478\n",
      "Epoch: 2, Batch: 8000, running_loss: 20.77540309354663\n",
      "Epoch: 2, Batch: 10000, running_loss: 20.896003486122936\n",
      "Epoch: 2, Batch: 12000, running_loss: 20.589116596616805\n",
      "Epoch: 2, Batch: 14000, running_loss: 21.581300710327923\n",
      "Epoch: 2, Batch: 16000, running_loss: 22.386829497292638\n",
      "Epoch: 2, Batch: 18000, running_loss: 21.46484287781641\n",
      "Epoch: 2, Batch: 20000, running_loss: 19.627930633723736\n",
      "Epoch: 2, Batch: 22000, running_loss: 20.192215147428215\n",
      "Epoch: 2, Batch: 24000, running_loss: 21.310492783784866\n",
      "Epoch: 2, Batch: 26000, running_loss: 20.488851111382246\n",
      "Epoch: 2, Batch: 28000, running_loss: 20.91997470986098\n",
      "Epoch: 2, Batch: 30000, running_loss: 20.750024852342904\n",
      "Epoch: 2, Batch: 32000, running_loss: 21.87753641232848\n",
      "Epoch: 2, Batch: 34000, running_loss: 20.697641723789275\n",
      "Epoch: 2, Batch: 36000, running_loss: 20.35222602635622\n",
      "Epoch: 2, Batch: 38000, running_loss: 21.245150060392916\n",
      "Epoch: 2, Batch: 40000, running_loss: 20.394743798300624\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "for epoch in range(8):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0,len(train_images), 4):\n",
    "        net.zero_grad()\n",
    "        out = net(train_images[i:i+4])\n",
    "        loss = criterion(out, train_labels[i:i+4])\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:\n",
    "            ## I recommend doing things like this. It helped me catch mistakes\n",
    "            #print(batch[0][0])\n",
    "            #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "            #plt.show()\n",
    "            #print(targets[0])\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * 0.01)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t_images[750], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8832 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = t_test_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-omaha",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
