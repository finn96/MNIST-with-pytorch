{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conventional-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by loading our data\n",
    "\n",
    "# Starting with the labels\n",
    "with open(\"./data/processed_training_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(labels) == 60000\n",
    "\n",
    "# Now for the images\n",
    "images = []\n",
    "with open(\"./data/processed_training_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        images.append(img)\n",
    "        \n",
    "# Again, let's do some random spot checking to make sure everything is as we expect\n",
    "assert len(images) == 60000\n",
    "i1,i2,i3 = np.random.randint(0, 60000, 3)\n",
    "assert images[i1].shape == (28,28)\n",
    "assert images[i2].shape == (28,28)\n",
    "assert images[i3].shape == (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And again, we'll just print out some images and their labels for good measure \n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,4,1)\n",
    "plt.text(9,40,f\"label: {labels[i1]}\")\n",
    "plt.imshow(images[i1], cmap='gray')\n",
    "fig.add_subplot(1,4,2)\n",
    "plt.text(10,40,f\"label: {labels[i2]}\")\n",
    "plt.imshow(images[i2], cmap='gray')\n",
    "fig.add_subplot(1,4,3)\n",
    "plt.text(11,40,f\"label: {labels[i3]}\")\n",
    "plt.imshow(images[i3], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "perfect-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Okay! Now the fun begins.\n",
    "# To start, let's just get everything over to Torch\n",
    "t_labels = torch.tensor(labels)\n",
    "t_images = torch.tensor(images)\n",
    "\n",
    "train_images = t_images.reshape(60000, 1, 784).float()\n",
    "train_labels = torch.zeros(60000, 1, 10)\n",
    "for i, label in enumerate(t_labels):\n",
    "    train_labels[i][0][label.item()] = 1\n",
    "\n",
    "#train_labels = train_labels.flip(0)\n",
    "#train_images = train_images.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "egyptian-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And we can take a look at their shapes\n",
    "import matplotlib.pyplot as plt\n",
    "z = np.random.randint(0, 60000)\n",
    "plt.imshow(train_images[z][0].reshape(28,28), cmap='gray')\n",
    "print(train_labels[z][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "pregnant-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !DF! Try just one fully connected layer\n",
    "# TODO: !DF! Write explanation on MSELoss()\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)  # stride = 1\n",
    "        #self.drop1 = nn.(389,p=.2) \n",
    "        self.fc1 = nn.Linear(782,100) # 80\n",
    "        #self.drop2 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(100, 10) # 80\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = F.max_pool1d(F.sigmoid(self.conv1(x)), (2))  ## TODO: Test this again against whatever you leave here\n",
    "        x = F.sigmoid(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        #x = self.drop2(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        return self.fc2(x)\n",
    "        #return self.fc3(x)\n",
    "\n",
    "## 100 -> ~76 ~78 ~78 ~78%\n",
    "## 80 -> ~78 ~76 ~ 77 ~78%\n",
    "\n",
    "## 100 with max_pool -> 71%, 73% 71%\n",
    "#https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + pytorch bias, Sigmoid, Kernel Size 3: 85%, 86%, 86%\n",
    "# 5 Epochs with pytorch weights/bias, Sigmoid, Kernel Size 3: 87%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights + 0 bias, Sigmoid, Kernel Size 3: 86%, 86%, 86%, 85%\n",
    "# 5 Epochs with +-[1/sqrt(n)] weights and bias, Sigmoid, Kernel Size 3: 86%, 85%, 86%\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 3: 87%, 87%, 86%,(5) -- 90.35% 90.44%  (10) **\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 5: 86%, 86%, 86% (5) -- 89.31% 90.03%  (10)\n",
    "#  5 Epochs with Normalized Xavier weights and 0 bias, Sigmoid, Kernel Size 8: 87%, 86%, 87% (5) -- 89.79% 88.58%  (10)\n",
    "\n",
    "## TODO: !DF! Best of the normalized Xavier head-to-head with pytorch defaults, 10 epochs, track loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "elegant-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "# https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "# TODO:!DF! test with and without these weights\n",
    "#net.fc1.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ = net.fc2.weight.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))\n",
    "#net.fc1.bias.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))\n",
    "#_ =  net.fc2.bias.data.uniform_(-1.0/np.sqrt(100),1.0/np.sqrt(100))-np.sqrt(6.0)\n",
    "net.fc1.bias.data.fill_(0)\n",
    "_ = net.fc2.bias.data.fill_(0)\n",
    "net.fc1.weight.data.uniform_(-np.sqrt(6.0)/np.sqrt(782+100),np.sqrt(6.0)/np.sqrt(782+100))\n",
    "_ = net.fc2.weight.data.uniform_(-np.sqrt(6.0)/np.sqrt(100+10),np.sqrt(6.0)/np.sqrt(100+10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "comprehensive-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch: 9, Batch: 58000, running_loss: 11.266556797199883'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0,len(train_images), 4):\n",
    "        net.zero_grad()\n",
    "        out = net(train_images[i:i+4])\n",
    "        loss = criterion(out, train_labels[i:i+4])\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:\n",
    "            ## I recommend doing things like this. It helped me catch mistakes\n",
    "            #print(batch[0][0])\n",
    "            #plt.imshow(batch[0][0].reshape((28,28)), cmap='gray')\n",
    "            #plt.show()\n",
    "            #print(targets[0])\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Epoch: {epoch}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * 0.01)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(t_images[750], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(t_images[750].flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sweet-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exotic-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "optical-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9044 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = t_test_images[9496]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(net, \"./models/1dC2fc85\")\n",
    "##torch.save(net.state_dict(), \"./models/1dC2fc85.state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.flip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = train_images[0][0]\n",
    "my_conv1d = nn.Conv1d(1,2,2)\n",
    "my_conv1d(my_img.unsqueeze(0).unsqueeze(0))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Conv1d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(train_images[0].unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "104/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stone-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0344, -0.0227, -0.0099,  ...,  0.0357,  0.0121, -0.0129],\n",
       "        [-0.0289, -0.0260, -0.0018,  ...,  0.0205, -0.0217,  0.0199],\n",
       "        [ 0.0091,  0.0305,  0.0191,  ..., -0.0006,  0.0047, -0.0305],\n",
       "        ...,\n",
       "        [-0.0057,  0.0190, -0.0134,  ..., -0.0067,  0.0096, -0.0161],\n",
       "        [ 0.0202, -0.0128, -0.0338,  ..., -0.0176, -0.0176,  0.0211],\n",
       "        [-0.0263, -0.0035,  0.0092,  ..., -0.0211, -0.0109, -0.0111]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = nn.Linear(782,100)\n",
    "ll2 = nn.Linear(782, 100)\n",
    "ll2.weight.data.uniform_(-1.0/np.sqrt(782),1.0/np.sqrt(782))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "requested-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6307e-05)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "neither-marking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.7258e-05)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll2.weight.data.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "southern-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "useful-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.zeros((5,15000,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "threaded-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15000, 1)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "christian-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 1)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-profit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
