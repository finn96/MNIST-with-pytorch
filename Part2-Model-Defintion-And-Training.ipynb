{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-affect",
   "metadata": {},
   "source": [
    "# Model Defintion and Training\n",
    "\n",
    "Now that we have parsed our training data, we can move on to defining our model. We will be using a number of different modules from pytorch:\n",
    "\n",
    "- **torch.nn**\n",
    "    - Provides the modules and classes we will use to define our network and its trainable layers\n",
    "- **torch.nn.Functional**\n",
    "    - Provides functional methods to compute functions that are stateless/not-trained (Activation functions, etc)\n",
    "- **torch**\n",
    "    - Used for creating and reshaping tensors throughout\n",
    "\n",
    "To start, we need to load in the data we parsed during part 1. If you have not gone through part 1 yet, you should do so before running this notebook; alternatively, you can add some cells to load the data in a custom manner if you have it stored somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is a little bit of ipy magic\n",
    "%store -r labels\n",
    "%store -r images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-adelaide",
   "metadata": {},
   "source": [
    "### A brief discussion of PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-function",
   "metadata": {},
   "source": [
    "Right now, the labels and images are stored as simple python lists. We'll use torch to convert them to tensors, which will allow us to easily apply more complex functions and transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabulous-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "training_labels = torch.tensor(labels)\n",
    "training_images = torch.tensor(images)\n",
    "print(type(training_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-point",
   "metadata": {},
   "source": [
    "You can see above that we've converted our list into an object of class `torch.Tensor`. We'll need to keep our data as this type during the training and evaluation periods of our model. \n",
    "\n",
    "Although they may seem weird at first, you'll find a lot of familiarity with Tensor classes if you've worked with numpy before. In fact, you can even iterate through them like regular old lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mysterious-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: tensor([1, 2, 3])\n",
      "Row 1: tensor([4, 5, 6])\n",
      "Row 2: tensor([7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "for i, row in enumerate(tensor):\n",
    "    print(f\"Row {i}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-migration",
   "metadata": {},
   "source": [
    "Similarily, if we wanted to iterate through the columns in our tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column 0: tensor([1, 4, 7])\n",
      " Column 1: tensor([2, 5, 8])\n",
      " Column 2: tensor([3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(tensor.T):\n",
    "    print(f\" Column {i}: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-flesh",
   "metadata": {},
   "source": [
    "I'm not going to go into too much more detail on tensor operations. Not only will you learn far more from the [documentation](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py), but I believe it's better to learn the power of tensors by seeing them in action.\n",
    "\n",
    "The one last thing I'll show you is in-place operations on tensors. This differs a bit from numpy, so I'm calling it out now as we will be using it later on in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greater-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_1: tensor([[1., 1., 1.]])\n",
      "tensor 2: tensor([[0., 0., 0.]])\n",
      "\n",
      "tensor_1: tensor([[0., 0., 0.]])\n",
      "tensor_2: tensor([[0., 0., 0.]])\n",
      "\n",
      "tensor_1: tensor([[-1., -1., -1.]])\n",
      "tensor_2: tensor([[-1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.ones(1,3)  \n",
    "tensor_2 = tensor_1.sub(1)    # Not in place. Returns a new tensor\n",
    "print(f\"tensor_1: {tensor_1}\")\n",
    "print(f\"tensor 2: {tensor_2}\\n\")\n",
    "\n",
    "tensor_2 = tensor_1.sub_(1)  # In place. Modifies the tensor and returns a reference\n",
    "print(f\"tensor_1: {tensor_1}\")\n",
    "print(f\"tensor_2: {tensor_2}\\n\")\n",
    "\n",
    "# Be careful capturing the output of in-place operations, as you must keep strict track of them to avoid unintentionally modifying data later on.\n",
    "\n",
    "tensor_2.sub_(1)\n",
    "print(f\"tensor_1: {tensor_1}\")  # We've also modified the original tensor variable\n",
    "print(f\"tensor_2: {tensor_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-citizen",
   "metadata": {},
   "source": [
    "### Re-shaping our data for the model\n",
    "\n",
    "I am going to be a bit of an oracle here, and tell you that our first pass at a torch neural network is going to expect the image data as a rank 1, 784 dimensional tensor. This is **not** the best way to learn from this data, as we are  totally blowing away any information about the vertical orientation of the pixels; but, it keeps things simple to start off with. We'll talk about 2d representation of the data later on.\n",
    "\n",
    "The other thing I will drag out of my crystal ball is that we will define our target label for any given input as a 10 dimensional vector (rank 1 tensor) where the value at every index is 0 except that of the target digit, which will be a 1.\n",
    "\n",
    "Now that we have our images and labels stored as `tensors`, these transformations become trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instant-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images.reshape(60000, 1, 784).float()  # 60000 images, 1 grayscale channel, 784 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-maldives",
   "metadata": {},
   "source": [
    "Note that if we ever want to display out one of these images, we'll have to reshape it back into something that plt can understand. That 'something' is a 2d numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imposed-fellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = torch.randint(len(training_images), (1,)).item()\n",
    "viewable_image = training_images[image_index].numpy().reshape(28,28)\n",
    "plt.imshow(viewable_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-expense",
   "metadata": {},
   "source": [
    "Now, let's \"reshape\" the label tensor into a new one that has a 10 dimensions vector for each label. Think about why I put \"reshape\" in quotes. In order to turn our label tensor from from a single 60000 dimensional vector to 60000 10 dimensional vectors, we are actually changing the \"space\" that our labels live in. This transformation changes the size of the space, so we need to make a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "seven-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = torch.zeros(60000, 10)            # Start everything as a zero\n",
    "for i, label in enumerate(training_labels):    # Enumerate through our labels and set the value at each index to 1\n",
    "    new_labels[i][label] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-laundry",
   "metadata": {},
   "source": [
    "Just for ease of use, we'll replace the original training_labels variable with the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frank-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = new_labels.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-device",
   "metadata": {},
   "source": [
    "### Building our  model\n",
    "Alright! We've got our training data in the shape and form we need it. Now we can go ahead and define our model. I'm not going to dive deep into the fundamentals of neural networks (I still have a lot to learn myself), but I'll share some guesses about our data that will inform our layer architecture.\n",
    "\n",
    "- Pixels in the flattened image will have a horizontal relationship to each other. In other words, it's probably valuable to know how the value of one pixel relates to the values of its neighbor(s). We can add a 1d convolutional layer to capture this information and identify whether certain \"features\" are present in locations along the flat image.\n",
    "\n",
    "\n",
    "- Once we have computed how pixels relate to their neighbors, we'll want to think about how these computed features can be reduced to smaller space. We can use a fully connected linear layer for this.\n",
    "\n",
    "\n",
    "- Finally, we want to take the output of that linear layer and squeeze it down even more, using the knowledge of the relationships between pixels and the \"features\" of the image they define to create our output vector which represents what digit the network thinks the image is\n",
    "\n",
    "##### Note:  practioners or students of classification neural networks will immediately know that this can be accomplished with a far less complicated model that realizes far better results. But we're going to use a convolutional layer because this is more about using pytorch than building the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-relation",
   "metadata": {},
   "source": [
    "#### The model class\n",
    "Our model class will inherit from the `nn.Module` class, which will automatically give us tools to track weights, biases, and gradients across our layers and easily perform feed-forward training and backpropogation adjustments.\n",
    "\n",
    "In our `__init__` method, we'll define the layers we mentioned above. We'll define just one more method, which is `forward` method, which represents what happens when an image makes a forward pass through our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abroad-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3)      # Convolutional layer. 1 input channel, 1 output channel, kernel size 3\n",
    "        self.fc1 = nn.Linear(782, 100)     # First linear layer. Takes the output of our convolution and reduces features\n",
    "        self.fc2 = nn.Linear(100,10)       # Second linear and final layer. Converts to image classification vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.conv1(x))       # Sigmoid activation of our conv layer\n",
    "        x = x.view(-1, x.shape[1:].numel())# Reshape for linear layers\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-island",
   "metadata": {},
   "source": [
    "There we have it. We've defined our model in about 10 lines. \n",
    "\n",
    "If you've used python before, it's likely very obvious how the above class implements to our proposed model architecture, but we'll do a quick overview to be sure.\n",
    "\n",
    "1. In the `__init__` method, we define our 3 layers as instance variables of our class: 1 Convolutional layer and 2 linear layers. It's important to set these layers here, so that pytorch's automatic gradient calculation (`autograd`) works as we expect.\n",
    "\n",
    "\n",
    "2. In the `forward` method, we simply move some input `x` through our layers by providing it as an input to each instance variable. We'll wrap these calls in the activation function of our choice when appropriate.\n",
    "\n",
    "If you're wondering about the `x = x.view(-1, x.shape[1:].numel())` step, you should know that it's reshaping the activations out of the convolutional layer to be the proper shape for our linear layers. PyTorch expects everything to have a batch dimension--which means that even when running our network on a single flattened grayscale image, the input shape is [1,1,784], not [1,784]. After our convolution, we want to flatten our activations to just be [1, a], where a is the number of elements after the peforming activation(convolution(input)) for a single image. You can extrapolate this to batched inputs, so the shape becomes [n, a], where n is the size of the mini-batch. Depending on your network architeture, this reshape may not always happen before your first linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-moore",
   "metadata": {},
   "source": [
    "### Training our Model\n",
    "Now that we've defined our model in a class, it's time to train.\n",
    "\n",
    "We start by just instantiating our model as we would any other python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "normal-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNISTNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-priority",
   "metadata": {},
   "source": [
    "#### The training loop\n",
    "We are going to write a lot of the training loop by hand, because I think it actually better shows the power of PyTorch. There are lots of powerful tools for training, like [DataLoaders](https://pytorch.org/docs/stable/data.html) and [optimizers](https://pytorch.org/docs/stable/optim.html), but using these right from the start sort of hides the inner awesomeness of PyTorch's `autograd` and `nn.Module` functionality. Also, writing it by hand is a good refresher of the concepts of how neural nets train without having to spend time writing all the backpropgation and gradient calculations yourself (thanks PyTorch).\n",
    "\n",
    "I'm just going to dump the whole loop here to start, and I'll break down the intersesting parts below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "operating-score",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch: 0, Batch: 6999, running_loss: 159.20936676114798'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a14ac83f5385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch-mnist/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch-mnist/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "MINI_BATCH = 4\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 3\n",
    "NUM_BATCHES = len(training_images) // MINI_BATCH\n",
    "net.train()\n",
    "for e in range(EPOCHS):\n",
    "    perm = torch.randperm(60000)\n",
    "    running_loss = 0.0\n",
    "    for i in range(NUM_BATCHES):\n",
    "        indexes = perm[i*MINI_BATCH:(i+1) * MINI_BATCH]\n",
    "        batch = training_images[indexes]\n",
    "        targets = training_labels[indexes]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        out = net(batch)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Epoch: {e}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-athletics",
   "metadata": {},
   "source": [
    "Let's talk about a few parts of this loop\n",
    "\n",
    "- ```criterion = nn.MSELoss()``` this is how we declare our cost/loss function. In this first pass at the network, we're using Mean Squared Error or Quadratic loss. When we do ```loss = criterion(out, targets)```, pytorch computes the loss for the given mini batch.\n",
    "\n",
    "\n",
    "- ```loss.backward()```. Here's where the magic starts. PyTorch keeps a graph of all operations carried out on all the parameters (weights, biases) for each layer in our network. When we call `loss.backward()`, PyTorch automatically calculates the gradient of our cost function w.r.t. the activations of our output layer. It then calculates the error of the output layer and backprops it through every path in the network, using it to compute the gradient for each weight and bias along the way.\n",
    "\n",
    "\n",
    "- ```\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * .01)\n",
    "``` \n",
    "here, we're manually performing stochagradient descent on the parameters (again, weights, biases) for each layer in our network. F.grad.data contains the gradients for each layer as computed by `loss.backward()`. If you're confused how this line updates the weights \n",
    "\n",
    "\n",
    "- ```net.zero_grad()``` PyTorch automatically accumulates gradients across multiple `loss.backward()` calls. This has some usefulness in RNNs or in cases where you run multiple batches before optimizing, but in our use case we don't want it. Summing the gradients of weights / biases w.r.t loss over multiple training/optimization steps will not help our network learn. We use the `zero_grad()` function to zero the gradient of each parameter at the start of each mini-batch training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "returning-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "foreign-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "drawn-lewis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8440 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "net.eval()\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-driving",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
