{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-affect",
   "metadata": {},
   "source": [
    "# Model Defintion and Training\n",
    "\n",
    "Now that we have parsed our training data, we can move on to defining our model. We will be using a number of different modules from pytorch:\n",
    "\n",
    "- **torch.nn**\n",
    "    - Provides the modules and classes we will use to define our network and its trainable layers\n",
    "- **torch.nn.Functional**\n",
    "    - Provides functional methods to compute functions that are stateless/not-trained (Activation functions, etc)\n",
    "- **torch**\n",
    "    - Used for creating and reshaping tensors throughout\n",
    "\n",
    "To start, we need to load in the data we parsed during part 1. If you have not gone through part 1 yet, you should do so before running this notebook; alternatively, you can add some cells to load the data in a custom manner if you have it stored somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is a little bit of ipy magic\n",
    "%store -r labels\n",
    "%store -r images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-adelaide",
   "metadata": {},
   "source": [
    "### A brief discussion of PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-function",
   "metadata": {},
   "source": [
    "Right now, the labels and images are stored as simple python lists. We'll use torch to convert them to tensors, which will allow us to easily apply more complex functions and transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fabulous-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "training_labels = torch.tensor(labels)\n",
    "training_images = torch.tensor(images)\n",
    "print(type(training_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-point",
   "metadata": {},
   "source": [
    "You can see above that we've converted our list into an object of class `torch.Tensor`. We'll need to keep our data as this type during the training and evaluation periods of our model. \n",
    "\n",
    "Although they may seem weird at first, you'll find a lot of familiarity with Tensor classes if you've worked with numpy before. In fact, you can even iterate through them like regular old lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mysterious-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: tensor([1, 2, 3])\n",
      "Row 1: tensor([4, 5, 6])\n",
      "Row 2: tensor([7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "for i, row in enumerate(tensor):\n",
    "    print(f\"Row {i}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-migration",
   "metadata": {},
   "source": [
    "Similarily, if we wanted to iterate through the columns in our tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column 0: tensor([1, 4, 7])\n",
      " Column 1: tensor([2, 5, 8])\n",
      " Column 2: tensor([3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(tensor.T):\n",
    "    print(f\" Column {i}: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-flesh",
   "metadata": {},
   "source": [
    "I'm not going to go into too much more detail on tensor operations. Not only will you learn far more from the [documentation](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py), but I believe it's better to learn the power of tensors by seeing them in action.\n",
    "\n",
    "The one last thing I'll show you is in-place operations on tensors. This differs a bit from numpy, so I'm calling it out now as we will be using it later on in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greater-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_1: tensor([[1., 1., 1.]])\n",
      "tensor 2: tensor([[0., 0., 0.]])\n",
      "\n",
      "tensor_1: tensor([[0., 0., 0.]])\n",
      "tensor_2: tensor([[0., 0., 0.]])\n",
      "\n",
      "tensor_1: tensor([[-1., -1., -1.]])\n",
      "tensor_2: tensor([[-1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "### TODO:!DF! make these one rank 1 so that the prints take up less space\n",
    "\n",
    "tensor_1 = torch.ones(1,3)  \n",
    "tensor_2 = tensor_1.sub(1)    # Not in place. Returns a new tensor\n",
    "print(f\"tensor_1: {tensor_1}\")\n",
    "print(f\"tensor 2: {tensor_2}\\n\")\n",
    "\n",
    "tensor_2 = tensor_1.sub_(1)  # In place. Modifies the tensor and returns a reference\n",
    "print(f\"tensor_1: {tensor_1}\")\n",
    "print(f\"tensor_2: {tensor_2}\\n\")\n",
    "\n",
    "# Be careful capturing the output of in-place operations, as you must keep strict track of them to avoid unintentionally modifying data later on.\n",
    "\n",
    "tensor_2.sub_(1)\n",
    "print(f\"tensor_1: {tensor_1}\")  # We've also modified the original tensor variable\n",
    "print(f\"tensor_2: {tensor_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-citizen",
   "metadata": {},
   "source": [
    "### Re-shaping our data for the model\n",
    "\n",
    "I am going to be a bit of an oracle here, and tell you that our first pass at a torch neural network is going to expect the image data as a rank 1, 784 dimensional tensor. This is **not** the best way to learn from this data, as we are  totally blowing away any information about the vertical orientation of the pixels; but, it keeps things simple to start off with. We'll talk about 2d representation of the data later on.\n",
    "\n",
    "The other thing I will drag out of my crystal ball is that we will define our target label for any given input as a 10 dimensional vector (rank 1 tensor) where the value at every index is 0 except that of the target digit, which will be a 1.\n",
    "\n",
    "Now that we have our images and labels stored as `tensors`, these transformations become trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "instant-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = training_images.reshape(60000, 1, 784).float()  # 60000 images, 1 grayscale channel, 784 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-maldives",
   "metadata": {},
   "source": [
    "Note that if we ever want to display out one of these images, we'll have to reshape it back into something that plt can understand. That 'something' is a 2d numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imposed-fellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANrUlEQVR4nO3db6ic5ZnH8d/PbCTG1qgNiSHGTbeIpgbWigaJdXWtVZu8iAX/RGSJrJgiFS3sixX3RYVlQRbbxTcWTlFyslRrRWOiiFVCNRtfqDG4x9jYmpVsmj94NohW0RCTXPviPCnHeOaek5ln5pmT6/uBw8w81zzzXAz55X5m7pm5HRECcOI7qekGAPQHYQeSIOxAEoQdSIKwA0n8VT8PZpu3/oEeiwhPtL2rkd32dbb/YHuH7Xu7eSwAveVO59ltT5P0R0nfl7Rb0huSbomI3xf2YWQHeqwXI/sSSTsi4v2IOCjp15JWdPF4AHqom7DPl/Sncbd3V9u+xPZq21tsb+niWAC61M0bdBOdKnzlND0ihiQNSZzGA03qZmTfLWnBuNtnS9rbXTsAeqWbsL8h6Vzb37R9sqSVkjbU0xaAunV8Gh8Rh2zfJem3kqZJejQi3qmtMwC16njqraOD8Zod6LmefKgGwNRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIdL9mMyVuyZEmx/vrrr/epk/6bPn16y9rBgweL+1599dXF+saNGzvqKauuwm57p6RPJB2WdCgiLq6jKQD1q2Nk//uI2F/D4wDoIV6zA0l0G/aQ9KLtN22vnugOtlfb3mJ7S5fHAtCFbk/jL4uIvbbnSHrJ9rsRsWn8HSJiSNKQJNmOLo8HoENdjewRsbe6HJW0TlL5bWcAjek47LZPtf31o9clXSNpW12NAahXN6fxcyWts330cR6LiBdq6WqKWblyZbG+du3aYv2ZZ54p1m+66abjbWlKiOBVXT91HPaIeF/S39bYC4AeYuoNSIKwA0kQdiAJwg4kQdiBJPiKaw02bdpUrD///PPF+vLly+ts54SxYsWKYp2vuB4fRnYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJ59hrs3bu3WH/55ZeL9Xnz5tXYzWA5cuRIy9quXbuK+y5atKjudlJjZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJJhn74NrrrmmWB8ZGelTJ/130kmtx5M5c+YU9505c2axfvrppxfrH330UbGeDSM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBPHsNzjnnnGJ96dKlxfqTTz5ZZzsD5dChQy1r7X5Pf9asWcU68+jHp+3IbvtR26O2t43bdqbtl2y/V12e0ds2AXRrMqfxayRdd8y2eyVtjIhzJW2sbgMYYG3DHhGbJH14zOYVkoar68OSrq+3LQB16/Q1+9yI2CdJEbHPdssPOdteLWl1h8cBUJOev0EXEUOShiTJdvT6eAAm1unU2we250lSdTlaX0sAeqHTsG+QtKq6vkrS+nraAdArbU/jbT8u6UpJs23vlvRTSQ9I+o3t2yXtknRjL5scdLfddluxftpppxXrmzdvrrGbwVL6Pvsll1xS3LfdPPqMGTOK9QMHDhTr2bQNe0Tc0qL0vZp7AdBDfFwWSIKwA0kQdiAJwg4kQdiBJPiKaw3OOuusYn39+vLHENauXVtnOwPl8OHDLWvr1q0r7nv33XcX6+2mNJl6+zJGdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ignn2Sbrooota1m6++ebivsPDw8V6RM4f8HnllVeK9Xbz7IsWLSrWP/3005a1e+65p7jvww8/XKx//PHHxfogYmQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYZ5+klStXtqzNnDmzuO8TTzxRdzt9M2dOy5W9JEmXX355sX7DDTe0rF111VUd9XTUCy+8UKzv37+/ZW3+/PnFfV999dVifdOmTcX6IGJkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkmGefpAsuuKBl7d133y3u+9prr9XdzpeccsopLWvLly8v7luaB5ekK664olifO3dusV76TnlpHlySZs+eXazv2rWrWH/xxRdb1trN8bd77Kmo7chu+1Hbo7a3jdt2v+09tt+q/pb1tk0A3ZrMafwaSddNsP0/IuLC6u/5etsCULe2YY+ITZI+7EMvAHqomzfo7rI9Up3mn9HqTrZX295ie0sXxwLQpU7D/gtJ35J0oaR9kn7W6o4RMRQRF0fExR0eC0ANOgp7RHwQEYcj4oikX0paUm9bAOrWUdhtzxt384eStrW6L4DB0Hae3fbjkq6UNNv2bkk/lXSl7QslhaSdkn7Uuxb7o933m5cuXdqy9uCDDxb3nTFjRrG+ePHiYv3GG28s1pctaz3zed555xX3bTefvGbNmmL92WefLdZHR0db1nbs2FHc97PPPivW2/22+0MPPVSsZ9M27BFxywSbH+lBLwB6iI/LAkkQdiAJwg4kQdiBJAg7kARfca3ccccdxfqsWbNa1q699trivrfeemuxfv755xfrhw8fLtYfe+yxlrU777yzuO/mzZuLdZw4GNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnm2Svtfu75888/b1lr95PH27dvL9afe+65Yr3dVzX37NlTrJ+ozj777KZbmFIY2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCUdE/w5m9+9gNSv9HHS775t/8cUXdbeTQrufkh4ZGSnWL7300jrbmTIiwhNtZ2QHkiDsQBKEHUiCsANJEHYgCcIOJEHYgST4PvskHThwoOkWcIytW7c23cKU0nZkt73A9u9sb7f9ju17qu1n2n7J9nvV5Rm9bxdApyZzGn9I0j9FxCJJl0r6se1vS7pX0saIOFfSxuo2gAHVNuwRsS8itlbXP5G0XdJ8SSskDVd3G5Z0fY96BFCD43rNbnuhpO9Iek3S3IjYJ439h2B7Tot9Vkta3WWfALo06bDb/pqkpyT9JCL+bE/4WfuviIghSUPVY0zZL8IAU92kpt5sT9dY0H8VEU9Xmz+wPa+qz5M02psWAdSh7cjusSH8EUnbI+Ln40obJK2S9EB1ub4nHQItnHzyyU23MKVM5jT+Mkn/IOlt229V2+7TWMh/Y/t2Sbsk3diTDgHUom3YI2KzpFYv0L9XbzsAeoWPywJJEHYgCcIOJEHYgSQIO5AEX3HFlLV48eKmW5hSGNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnm2dGYhQsXFuvTpk3rTyNJMLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBLMs6Mx7ZbBjigvIDQ8PFys48sY2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCbeby7S9QNJaSWdJOiJpKCIesn2/pDsk/V911/si4vk2j1U+GICuRcSEqy5PJuzzJM2LiK22vy7pTUnXS7pJ0qcR8eBkmyDsQO+1Cvtk1mffJ2lfdf0T29slza+3PQC9dlyv2W0vlPQdSa9Vm+6yPWL7UdtntNhnte0ttrd01yqAbrQ9jf/LHe2vSXpF0r9FxNO250raLykk/avGTvX/sc1jcBoP9FjHr9klyfZ0Sc9J+m1E/HyC+kJJz0VEcaU9wg70Xquwtz2Nt21Jj0jaPj7o1Rt3R/1Q0rZumwTQO5N5N/67kv5L0tsam3qTpPsk3SLpQo2dxu+U9KPqzbzSYzGyAz3W1Wl8XQg70Hsdn8YDODEQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuj3ks37Jf3vuNuzq22DaFB7G9S+JHrrVJ29/XWrQl+/z/6Vg9tbIuLixhooGNTeBrUvid461a/eOI0HkiDsQBJNh32o4eOXDGpvg9qXRG+d6ktvjb5mB9A/TY/sAPqEsANJNBJ229fZ/oPtHbbvbaKHVmzvtP227beaXp+uWkNv1Pa2cdvOtP2S7feqywnX2Guot/tt76meu7dsL2uotwW2f2d7u+13bN9TbW/0uSv01Zfnre+v2W1Pk/RHSd+XtFvSG5JuiYjf97WRFmzvlHRxRDT+AQzbfyfpU0lrjy6tZfvfJX0YEQ9U/1GeERH/PCC93a/jXMa7R721Wmb8NjX43NW5/HknmhjZl0jaERHvR8RBSb+WtKKBPgZeRGyS9OExm1dIGq6uD2vsH0vftehtIETEvojYWl3/RNLRZcYbfe4KffVFE2GfL+lP427v1mCt9x6SXrT9pu3VTTczgblHl9mqLuc03M+x2i7j3U/HLDM+MM9dJ8ufd6uJsE+0NM0gzf9dFhEXSfqBpB9Xp6uYnF9I+pbG1gDcJ+lnTTZTLTP+lKSfRMSfm+xlvAn66svz1kTYd0taMO722ZL2NtDHhCJib3U5Kmmdxl52DJIPjq6gW12ONtzPX0TEBxFxOCKOSPqlGnzuqmXGn5L0q4h4utrc+HM3UV/9et6aCPsbks61/U3bJ0taKWlDA318he1TqzdOZPtUSddo8Jai3iBpVXV9laT1DfbyJYOyjHerZcbV8HPX+PLnEdH3P0nLNPaO/P9I+pcmemjR199I+u/q752me5P0uMZO677Q2BnR7ZK+IWmjpPeqyzMHqLf/1NjS3iMaC9a8hnr7rsZeGo5Ieqv6W9b0c1foqy/PGx+XBZLgE3RAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMT/AyEhHYFHXuqAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = torch.randint(len(training_images), (1,)).item()\n",
    "viewable_image = training_images[image_index].numpy().reshape(28,28)\n",
    "plt.imshow(viewable_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-expense",
   "metadata": {},
   "source": [
    "Now, let's \"reshape\" the label tensor into a new one that has a 10 dimensions vector for each label. Think about why I put \"reshape\" in quotes. In order to turn our label tensor from from a single 60000 dimensional vector to 60000 10 dimensional vectors, we are actually changing the \"space\" that our labels live in. This transformation changes the size of the space, so we need to make a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "seven-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = torch.zeros(60000, 10)            # Start everything as a zero\n",
    "for i, label in enumerate(training_labels):    # Enumerate through our labels and set the value at each index to 1\n",
    "    new_labels[i][label] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-laundry",
   "metadata": {},
   "source": [
    "Just for ease of use, we'll replace the original training_labels variable with the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "frank-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = new_labels.float()\n",
    "#training_images = training_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-device",
   "metadata": {},
   "source": [
    "### Building our  model\n",
    "Alright! We've got our training data in the shape and form we need it. Now we can go ahead and define our model. I'm not going to dive deep into the fundamentals of neural networks (I still have a lot to learn myself), but I'll share some guesses about our data that will inform our layer architecture.\n",
    "\n",
    "- Pixels in the flattened image will have a horizontal relationship to each other. In other words, it's probably valuable to know how the value of one pixel relates to the values of its neighbor(s). We can add a 1d convolutional layer to capture this information and identify whether certain \"features\" are present in locations along the flat image.\n",
    "\n",
    "\n",
    "- Once we have computed how pixels relate to their neighbors, we'll want to think about how these computed features can be reduced to smaller space. We can use a fully connected linear layer for this.\n",
    "\n",
    "\n",
    "- Finally, we want to take the output of that linear layer and squeeze it down even more, using the knowledge of the relationships between pixels and the \"features\" of the image they define to create our output vector which represents what digit the network thinks the image is\n",
    "\n",
    "##### Note:  practioners or students of classification neural networks will immediately know that this can be accomplished with a far less complicated model that realizes far better results. But we're going to use a convolutional layer because this is more about using pytorch than building the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-relation",
   "metadata": {},
   "source": [
    "#### The model class\n",
    "Our model class will inherit from the `nn.Module` class, which will automatically give us tools to track weights, biases, and gradients across our layers and easily perform feed-forward training and backpropogation adjustments.\n",
    "\n",
    "In our `__init__` method, we'll define the layers we mentioned above. We'll define just one more method, which is `forward` method, which represents what happens when an image makes a forward pass through our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abroad-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,1,3) # Convolutional layer. 1 input channel, 1 output channel, kernel size 3\n",
    "        self.fc1 = nn.Linear(782, 100)     # First linear layer. Takes the output of our convolution and reduces features\n",
    "        self.fc2 = nn.Linear(100,10)       # Second linear and final layer. Converts to image classification vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.conv1(x))       # Sigmoid activation of our conv layer\n",
    "        x = x.view(-1, x.shape[1:].numel())# Reshape for linear layers\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-island",
   "metadata": {},
   "source": [
    "There we have it. We've defined our model in about 10 lines. \n",
    "\n",
    "If you've used python before, it's likely very obvious how the above class implements to our proposed model architecture, but we'll do a quick overview to be sure.\n",
    "\n",
    "1. In the `__init__` method, we define our 3 layers as instance variables of our class. (( If you're confused about the arguments passed to these layers, you can think of them as representing the size of the input and output of the layer; but, you should probably do some outside reading to catch up on this.))\n",
    "\n",
    "\n",
    "2. In the `forward` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "normal-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNISTNet()\n",
    "net = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "operating-score",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epoch: 9, Batch: 14999, running_loss: 30.332845590543002'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "MINI_BATCH = 4\n",
    "criterion = nn.MSELoss()\n",
    "EPOCHS = 3\n",
    "NUM_BATCHES = len(training_images) // MINI_BATCH\n",
    "net.train()\n",
    "#perm = torch.randperm(60000)\n",
    "for e in range(EPOCHS):\n",
    "    perm = torch.randperm(60000)\n",
    "    running_loss = 0.0\n",
    "    for i in range(NUM_BATCHES):\n",
    "        indexes = perm[i*MINI_BATCH:(i+1) * MINI_BATCH]\n",
    "        batch = training_images[indexes]\n",
    "        targets = training_labels[indexes]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        out = net(batch)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            clear_output(wait=True)\n",
    "            display(f\"Epoch: {e}, Batch: {i}, running_loss: {running_loss}\")\n",
    "            running_loss = 0.0\n",
    "        for f in net.parameters():\n",
    "            f.data.sub_(f.grad.data * .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "returning-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"./data/processed_testing_labels.csv\") as labels_file:\n",
    "    labels_string = labels_file.read()\n",
    "    testing_labels = np.array(labels_string.split(','), dtype=int)\n",
    "    \n",
    "# Recall we had 60000 images. Let's make sure we didn't lose anythin\n",
    "assert len(testing_labels) == 10000\n",
    "\n",
    "# Now for the images\n",
    "testing_images = []\n",
    "with open(\"./data/processed_testing_images\") as images_file:\n",
    "    raw_image_strings = images_file.readlines()\n",
    "    for img_string in raw_image_strings:\n",
    "        img_flat = np.array(img_string.split(\",\"), dtype=np.double)\n",
    "        img = np.reshape(img_flat, (28,28))\n",
    "        testing_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "foreign-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_labels = torch.tensor(testing_labels)\n",
    "t_test_images = torch.tensor(testing_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "drawn-lewis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8900 / 10000\n"
     ]
    }
   ],
   "source": [
    "test_imgs = t_test_images\n",
    "test_labels = t_test_labels\n",
    "correct = []\n",
    "net.eval()\n",
    "for i,img in enumerate(test_imgs):\n",
    "    res = torch.argmax(net(img.flatten().float().unsqueeze(0).unsqueeze(0)))\n",
    "    targ = test_labels[i]\n",
    "    if res == targ:\n",
    "        correct.append(1)\n",
    "    else:\n",
    "        correct.append(0)\n",
    "\n",
    "print(f\"{sum(correct)} / {len(correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-driving",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
