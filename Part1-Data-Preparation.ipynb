{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tight-kitty",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The first step of this walkthrough is getting our hands on the data that will train and test our PyTorch model. The data is available from a number of sources, including websites like [kaggle](https://www.kaggle.com/c/digit-recognizer/data) and [Louisiana State University's website](https://csc.lsu.edu/~saikat/n-mnist/). It's even made available directly through [torchvision](https://pytorch.org/vision/0.8/datasets.html#mnist).\n",
    "\n",
    "We are going to go right to the source on this one, and download it directly from [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). The dataset has become quite widely used in the world of image classification, and it was popularized after Yann's 1998 Paper [Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/document/726791).\n",
    "\n",
    "## Data Sources (word?)\n",
    "There are 4 files we need to train our network:\n",
    "- The set of training images:  http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "- The set of training labels:  http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "- The set of testing images:   http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "- The set of testing labels:   http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "To download them, I recommend simply (from the root level of this repository):\n",
    "```\n",
    "$> mkdir data\n",
    "$> wget --directory-prefix data http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "$> wget --directory-prefix data http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "$> wget --directory-prefix data http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "$> wget --directory-prefix data http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "```\n",
    "\n",
    "The files are gunzipped, so we'll need to uncompress them. We can do so in one line:\n",
    "```\n",
    "$> gunzip data/*\n",
    "```\n",
    "\n",
    "Now we have all the data we need on our local machine. Let's start out by taking a look at the data and figuring out how to parse it.\n",
    "\n",
    "## Data Parsing and Loading\n",
    "\n",
    "*Aside: Many of you may wonder: why go through all the trouble of parsing the dataset when it is so easily available in nicer formats from other sources. (As mentioned before, `torchvision` has this dataset built in and ready to be used with a `TrainLoader`.) There's even a python library called `python-mnist` that can automatically parse these files for you. I chose to include this portion of the walkthrough because I myself found it of great educational value to learn how to parse the data from raw, unsigned bytes. In the real world of machine learning, your source data is going to start off messy. It's not going to be already built in to your ML framework where you can download it and create a fancy training iterator with a single function call. I believe that learning to parse the data yourself is good practice for when it comes time to use machine learning in your own projects, on your own data, and in pursuit of your interests.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-chapter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
